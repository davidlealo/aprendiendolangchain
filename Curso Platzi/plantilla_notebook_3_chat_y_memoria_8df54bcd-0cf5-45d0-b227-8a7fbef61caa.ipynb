{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Uso de modelos de chat con LangChain\n",
        "\n",
        "Un modelo de chat es una variante especializada de los modelos de lenguaje, diseñado y optimizado para manejar interacciones conversacionales. Estos modelos son el núcleo detrás de aplicaciones como los chatbots, asistentes virtuales y cualquier otra aplicación que requiera interacción en lenguaje natural.\n",
        "\n",
        "En este ámbito, los modelos de chat trabajan con tres tipos de mensajes distintos:\n",
        "\n",
        "1. `SystemMessage`: Este tipo de mensaje establece el comportamiento y los objetivos del Modelo de Lenguaje Masivos (LLM, por sus siglas en inglés). En `SystemMessage` se pueden plasmar instrucciones específicas tales como \"Actuar como un gerente de marketing\" o \"Devolver solo una respuesta JSON sin texto explicativo\".\n",
        "\n",
        "2. `HumanMessage`: Este es el espacio donde se ingresan las instrucciones o consultas del usuario que posteriormente serán enviadas al LLM.\n",
        "\n",
        "3. `AIMessage`: Aquí es donde se almacenan las respuestas generadas por el LLM. Este tipo de mensaje es importante para conservar el historial de chat, que luego será utilizado para enviar al LLM en futuras solicitudes y así mantener el contexto de la conversación.\n",
        "\n",
        "Existe también un tipo genérico de mensaje denominado `ChatMessage`, que acepta una entrada de \"rol\" arbitraria y puede ser utilizado en circunstancias que requieran roles distintos a los tres mencionados previamente (System, Human, AI). Sin embargo, en la mayoría de los casos, se utilizan principalmente los tres tipos de mensajes mencionados anteriormente.\n"
      ],
      "metadata": {
        "id": "eY6RRzv8idq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai langchain"
      ],
      "metadata": {
        "id": "MCfvGYC1izEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass('Enter the secret value: ')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPzL6abai2Dv",
        "outputId": "1b9ef077-5c36-4cea-d62d-86ea39c9f96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat_gpt3_5 = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "aAa1Zx2rifFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage(content=\"Eres un asistente en un Call Center de reparación de lavadoras.\"),\n",
        "    HumanMessage(content=\"Cómo estás? Necesito ayuda.\"),\n",
        "    AIMessage(content=\"Estoy bien, gracias. En qué puedo ayudar?\"),\n",
        "    HumanMessage(content=\"Quiero reparar mi lavadora.\")\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "hWa6uZ5FifL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbyKDOJBifQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En sí mismo, res es un AIMessage."
      ],
      "metadata": {
        "id": "F5VxsplekmLi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TwAFO_DifUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos seguir el chat..."
      ],
      "metadata": {
        "id": "ev49bl2ilWXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append el res a nuestra serie de mensajes\n",
        "\n",
        "\n",
        "# Agregamos un nuevo mensaje del humano\n",
        "\n",
        "\n",
        "# send to chat-gpt\n"
      ],
      "metadata": {
        "id": "gu78KLj7ifYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCOqtjcslU6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Chat prompt templates\n",
        "\n",
        "Las plantillas de prompts en LangChain proporcionan una forma flexible y dinámica de interactuar con los LLM. En lugar de codificar prompts estáticos, estas plantillas permiten la incorporación de entradas variables del usuario.\n"
      ],
      "metadata": {
        "id": "nMXBlfQzmFSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "plantilla_sistema=\"\"\"\n",
        "Eres un experto en productos que debe proporcionar información detallada sobre productos de la marca {marca}.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ5I1UlqmLLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Memoria en LangChain\n",
        "\n",
        "Comprender cómo se utiliza la memoria en LangChain es fundamental para construir chat efectivos. Su papel vital está en cómo se puede emplear para hacer que tus modelos sean más conversacionales y parecidos a los humanos.\n",
        "\n",
        "## 2.1 La importancia de la memoria\n",
        "\n",
        "A menudo se espera que los modelos de lenguaje interactúen como un conversador humano. Los usuarios asumen que el modelo recordará partes anteriores de la conversación, entenderá el contexto y responderá en consecuencia. Sin embargo, los grandes modelos de lenguaje como GPT-3 o GPT-4, en su esencia, son sin estado, es decir, no poseen una memoria inherente.\n",
        "\n",
        "La memoria se vuelve vital en casos en los que un usuario se refiere a temas discutidos anteriormente o utiliza pronombres para referirse a una entidad mencionada anteriormente. Este fenómeno se conoce como _resolución de correferencia_ y es un desafío clave en el procesamiento del lenguaje natural.\n",
        "\n",
        "```python\n",
        "Ejemplo:\n",
        "Usuario: \"Conocí a un chico llamado John ayer. Él es futbolista.\"\n",
        "```\n",
        "\n",
        "Aquí, \"Él\" se refiere a \"John\". El modelo necesita memoria para resolver esta referencia.\n",
        "\n",
        "## 2.2 Enfoques para la gestión de la memoria\n",
        "\n",
        "Hay dos enfoques principales para la gestión de la memoria en LangChain:\n",
        "\n",
        "1. Incorporar la memoria en la indicación.\n",
        "2. Utilizar un mecanismo de búsqueda externo.\n",
        "\n",
        "Profundizaremos en ambos enfoques en las siguientes secciones.\n",
        "\n",
        "### 2.2.1 Incorporar la memoria en la indicación\n",
        "\n",
        "La forma más sencilla de incorporar la memoria es incluir el historial de conversación en la indicación. Por ejemplo, consideremos una conversación con un asistente digital llamado 'Kate'.\n",
        "\n",
        "```python\n",
        "Contexto: Eres un asistente digital al que le gusta tener conversaciones llamado Kate.\n",
        "Usuario: Hola, soy Sam.\n",
        "Agente: Hola Sam, ¿cómo estás?\n",
        "Usuario: Bien, ¿cuál es tu nombre?\n",
        "Agente: Mi nombre es Kate.\n",
        "Usuario: ¿Cómo estás hoy, Kate?\n",
        "```\n",
        "Este método implica agregar el historial de conversación a la indicación. El historial de conversación actúa como contexto para el modelo, permitiéndole comprender la discusión en curso. Sin embargo, este método tiene limitaciones. Dado que los modelos de lenguaje actuales como GPT-4 tienen un límite de tokens (por ejemplo, 4096 tokens para GPT-4), las conversaciones extensas no cabrían en este límite.\n",
        "\n",
        "### 2.2.2 Búsqueda externa\n",
        "\n",
        "Una alternativa a incrustar la memoria en la indicación es utilizar una búsqueda externa, como una base de datos o un grafo de conocimiento. Este método, aunque más complejo, puede manejar conversaciones más grandes y dinámicas que cubriremos en futuros capítulos.\n",
        "\n",
        "## 2.3 Estrategias de gestión de memoria integradas en LangChain\n",
        "\n",
        "LangChain ofrece estrategias integradas para gestionar la memoria de manera efectiva:\n",
        "\n",
        "1. Inserción directa en la indicación: Como se discutió en la sección anterior, esta estrategia simplemente agrega el historial de conversación a la indicación.\n",
        "2. Resumen de la conversación: Este método implica generar un resumen de la conversación e incluirlo en la indicación. El resumen en sí se realiza mediante un modelo de lenguaje separado.\n",
        "2. Memoria de ventana: Aquí, solo se incluyen los últimos intercambios de la conversación en la indicación.\n",
        "4. Mezcla de estrategias: Esta estrategia involucra una combinación de los últimos intercambios y una versión resumida del resto de la conversación.\n",
        "\n",
        "Además, LangChain te permite personalizar estas estrategias o incluso implementar las tuyas propias, brindándote flexibilidad según los requisitos únicos de tu caso de uso.\n",
        "\n",
        "Recuerda, la gestión de la memoria es una parte clave en la creación de modelos interactivos y atractivos. La estrategia que elijas puede afectar significativamente el rendimiento del modelo y la experiencia del usuario. Elige sabiamente y no temas experimentar e iterar para encontrar el enfoque más adecuado.\n"
      ],
      "metadata": {
        "id": "l6OSrlH67214"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 ConversationBufferMemory: conversaciones cortas y el enfoque ingenuo\n",
        "\n",
        "Vamos a sumergirnos en la implementación práctica de la memoria en LangChain. Primero se debe configurar el entorno mediante la instalación de paquetes necesarios como OpenAI y LangChain."
      ],
      "metadata": {
        "id": "esluWgNJ-XRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip -q install openai langchain"
      ],
      "metadata": {
        "id": "An8gnZlw8cYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter the secret value: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrTppQP679f3",
        "outputId": "4773fcbf-c1ca-4d4d-d253-c8ee8f33b9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret value: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, nos centraremos en el tipo de memoria más sencillo, la \"Memoria del Búfer de Conversación\" (Conversation Buffer Memory, en inglés). Este tipo de memoria almacena la historia de la conversación directamente en el prompt, a medida que la conversación progresa.\n",
        "\n",
        "En LangChain, el proceso de implementación es sencillo. Primero importamos el tipo de memoria de LangChain y lo instanciamos. Luego, pasamos esta instancia de memoria a nuestro modelo de lenguaje 'chain'.\n",
        "\n",
        "Usaremos una simple \"ConversationChain\" para este ejemplo. Esta cadena nos permite conversar con un modelo llamando a la función 'conversation_predict' y pasando nuestro texto de entrada."
      ],
      "metadata": {
        "id": "kd_xAoA8-tMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "piuGz1sa-Bte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name='text-davinci-003',\n",
        "             temperature=0,\n",
        "             max_tokens = 256,\n",
        "             )"
      ],
      "metadata": {
        "id": "YhGkzF_s_CT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qdUJAwq_Mn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNjPLFKR_OXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, hemos establecido `verbose=True`. Esto nos permite ver el mensaje completo y la respuesta del modelo cada vez que hacemos una predicción. El mensaje consiste en la conversación actual. Para la primera predicción, es simplemente \"Hola, soy Sam\", y la IA responde en consecuencia."
      ],
      "metadata": {
        "id": "kUMkRKqs_a_V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLYHBUVf_RSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfDZ4Zg-_iIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A medida que la conversación continúa, es importante tener en cuenta que nuestro búfer de conversación va acumulando nuestro historial conversacional. Lo que ocurre es que el mensaje que se introduce en el modelo de lenguaje se amplía con cada turno de la conversación. Este método funciona bien para conversaciones cortas, pero para diálogos extensos, puede llegar a ser demasiado largo para caber en el límite de tokens del modelo de lenguaje. Esta es una limitación que discutiremos a continuación.\n",
        "\n",
        "Este enfoque de memoria intermedia de conversación, la forma más simple de memoria en LangChain, es sorprendentemente eficaz. Es particularmente adecuado para escenarios donde hay un número predeterminado de interacciones con el bot, o lo has programado de tal manera que después de un cierto número de turnos (digamos cinco), la conversación termina. En estos casos, la memoria intermedia de conversación servirá perfectamente a tus propósitos."
      ],
      "metadata": {
        "id": "Q6zmbvOeAg0x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKzo57p0AHOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "33Q6izpAAZGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 ConversationBufferWindowMemory - ¿Cuántas interacciones debemos recordar?\n",
        "\n",
        "El sistema de `Memoria de Ventana del Búfer de Conversación`(`Conversation Buffer Window Memory`, en inglés) es algo similar a la `Memoria del Búfer de Conversación`, pero solo mantiene las últimas 'k' interacciones en la indicación. El parámetro 'k' se puede ajustar según tus preferencias y restricciones presupuestarias."
      ],
      "metadata": {
        "id": "SCV97KWIJMeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "avw1wFJKJLgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKaVzNvEJWU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tTyqPIiSJbah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Que ondi, como etai? Soy Omar y escribo muy coloquial.\")"
      ],
      "metadata": {
        "id": "uuKuPQg6JgQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Estoy buscando que me hables coloquialmente pues hablar formal es aprehender mi libertad de expresión.\")"
      ],
      "metadata": {
        "id": "sFeBIIo4JjrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Sobre la libertad de mi pueblo latinoamericano\")"
      ],
      "metadata": {
        "id": "25bK2bLmJxo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Que no estoy haciendo lo suficiente.\")"
      ],
      "metadata": {
        "id": "XlYDAAZIJ5Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"¿Cómo me gusta escribir? Coloquial? O formal?\")"
      ],
      "metadata": {
        "id": "KcsAYftdLC8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The memory buffer shows every interaction no matter the k."
      ],
      "metadata": {
        "id": "7QmqXv1ULqdI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFK6C9W6J_RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este enfoque de gestión de memoria alimenta únicamente las últimas `k` interacciones en el Modelo de Lenguaje Grande, lo cual puede ser una táctica beneficiosa al interactuar con el bot. Por ejemplo, si estableces `k=5`, la mayoría de las conversaciones no requerirán hacer referencia a partes anteriores de la conversación. Es posible dar la impresión de una memoria a largo plazo a los usuarios simplemente manteniendo un recuerdo de los últimos tres a cinco pasos de la conversación.\n",
        "\n",
        "Sin embargo, aunque la Memoria de Ventana del Búfer de Conversación solo suministra al Modelo de Lenguaje Grande las últimas 'k' interacciones, aún conserva toda la conversación. Esto es útil para inspeccionar el historial de la conversación o almacenarlo para futura referencia."
      ],
      "metadata": {
        "id": "Ytt6SNm4LVZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 ConversationSummaryMemory: Crea un resumen de nuestras interacciones\n",
        "\n",
        "Otro tipo de gestión de memoria en LangChain es la `Memoria de Resumen de Conversación` (`Conversation Summary Memory`, en inglés). La distinción clave aquí es que en lugar de mantener toda la conversación, LangChain la resume."
      ],
      "metadata": {
        "id": "iZKt9mFuBIaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain"
      ],
      "metadata": {
        "id": "rIZL1XYRCTVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name='text-davinci-003',\n",
        "             temperature=0,\n",
        "             max_tokens = 256,)"
      ],
      "metadata": {
        "id": "8E74b9QnCWFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzMdM6gXCccQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FfYhQxvJC7Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, en lugar de transferir cada ida y vuelta entre el usuario y el bot al prompt, el sistema genera un resumen. Observa que, en esta fase, la versión resumida utiliza más tokens que la conversación sin procesar. Sin embargo, a medida que la conversación continúa, el resumen se vuelve más eficiente en cuanto a tokens."
      ],
      "metadata": {
        "id": "vket5xfpDlQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Que ondi, como etai? Soy Omar y escribo muy coloquial.\")"
      ],
      "metadata": {
        "id": "5o7sg81vDBFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Estoy muy bien. Cuéntame sobre la revolución de las naciones latinoamericanas y la historia de la opresión indígena.\")"
      ],
      "metadata": {
        "id": "571bUZjYDNeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Entonces por qué sigue habiendo racismo en la región? Habla coloquial para poderme identificar contigo.\")"
      ],
      "metadata": {
        "id": "dZPlLHEND23K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwZCVrf-DfOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado proporciona una versión resumida de la conversación hasta ahora, es decir, el humano (Omar) se presenta, pregunta cómo está el AI y solicita ayuda con el soporte al cliente. El AI (llamado AI) saluda a Sam y pregunta qué lo trae aquí hoy. En respuesta a la solicitud de Sam, el AI está dispuesto a ayudar y pregunta qué tipo de soporte se necesita.\n",
        "\n",
        "Es importante tener en cuenta que el sistema de resumen mantiene una referencia constante a 'AI' y 'Omar', en lugar de usar pronombres como 'él', 'ella' o 'ello'. Este enfoque asegura claridad y ayuda a evitar posibles confusiones debido a la interpretación errónea de los pronombres."
      ],
      "metadata": {
        "id": "x-s1MwH6DwRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 ConversationSummaryBufferMemory - Crea un resumen a partir de cierto número de interacciones pasadas\n",
        "\n",
        "El cuarto tipo de memoria es la `Memoria de Ventana de Resumen del Búfer` (`Summary Buffer Window Memory`, en inglés). Esta estrategia de memoria es una combinación de los dos primeros tipos que hemos visto, ya que incluye un aspecto de resumen y también mantiene un búfer de un número fijo de tokens. Esta estrategia dual permite equilibrar entre mantener un resumen completo de la conversación y reducir el uso de tokens para un funcionamiento más eficiente.\n"
      ],
      "metadata": {
        "id": "Xg_k49PIuTHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory"
      ],
      "metadata": {
        "id": "Id6pOzCouuPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = OpenAI(model_name='text-davinci-003',\n",
        "             temperature=0,\n",
        "             max_tokens = 512,\n",
        "            verbose=True)"
      ],
      "metadata": {
        "id": "KjDIQXWLvYy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "tPllDwrzvqyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "# Setting k=2, will only keep the last 2 interactions in memory\n",
        "# max_token_limit=40 - token limits needs transformers installed\n",
        "memory = ConversationSummaryBufferMemory(llm=OpenAI(), max_token_limit=40, k=2)"
      ],
      "metadata": {
        "id": "aTIX7O8TvelJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "F0PVOAe2vg2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las interacciones iniciales de la conversación seguirán un patrón similar al anterior. Sin embargo, una vez que la interacción supera el límite del búfer (es decir, el total de tokens supera el límite de 40 tokens), la memoria empieza a resumir las primeras interacciones, mientras mantiene el texto completo de las interacciones recientes."
      ],
      "metadata": {
        "id": "GCm0R0MXv_m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"Que ondi, como etai? Soy Omar y escribo muy coloquial.\")"
      ],
      "metadata": {
        "id": "3s5F9rAGviRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, conserva el texto completo de las interacciones recientes hasta que el recuento total de tokens alcance nuevamente el límite. En este punto, la parte más temprana de la interacción se resume aún más o se elimina, y el proceso continúa. Esto permite que el bot conserve las partes más relevantes y recientes de la conversación en detalle completo, al tiempo que tiene un contexto resumido de las partes anteriores."
      ],
      "metadata": {
        "id": "B51eXHaqwKZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"Estoy muy bien. Cuéntame sobre la revolución de las naciones latinoamericanas y la historia de la opresión indígena.\")"
      ],
      "metadata": {
        "id": "kACKur3xwJgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"Entonces por qué sigue habiendo racismo en la región? Habla coloquial para poderme identificar contigo.\")"
      ],
      "metadata": {
        "id": "lz7UD-EVwV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"¿Cuál fue la primera pregunta que te hice?\")"
      ],
      "metadata": {
        "id": "v5Pp4gpYwna_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts para cadenas con memoria\n",
        "\n",
        "Podemos examinar el prompt que está recibiendo nuestra cadena por default y alterarlo a nuestro gusto."
      ],
      "metadata": {
        "id": "ylXavRwAFbCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.prompt"
      ],
      "metadata": {
        "id": "8J3VQVqiFqQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation_with_summary.prompt.template)"
      ],
      "metadata": {
        "id": "0wWDoBbcFrVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identificamos que el prompt que usemos tiene que tener dos inputs: history e input. Con ello podemos diseñar nuestro prompt."
      ],
      "metadata": {
        "id": "3FnRsgrSGH1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "plantilla_chilena = \"\"\"\n",
        "La siguiente es una conversación entre un humano y una inteligencia artificial, compadre.\n",
        "Esta IA, cachai, es un asistente de ventas de autos, bien metido en el tema.\n",
        "Si la IA no cachara alguna respuesta, no va a tener atados pa' decir que no sabe, ah.\n",
        "La IA va al hueso, sin rodeos. Pregunta directamente qué necesita con lo referente al auto.\n",
        "La IA habla con una forma popular chilena y parecida a memes.\n",
        "\n",
        "Conversación actual:\n",
        "{history}\n",
        "Humano: {input}\n",
        "IA:\n",
        "\"\"\"\n",
        "\n",
        "PLANTILLA_CHILENA_CONVERSACION_RESUMEN = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=plantilla_chilena\n",
        ")"
      ],
      "metadata": {
        "id": "8CoLZlGaGCOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_chileno = ConversationChain(\n",
        "    llm=llm,\n",
        "      memory=memory,\n",
        "    verbose=True,\n",
        "    prompt=PLANTILLA_CHILENA_CONVERSACION_RESUMEN\n",
        ")"
      ],
      "metadata": {
        "id": "2wQXgrJbHh1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_chileno.predict(\n",
        "    input=\"¿Oli como estai?\"\n",
        "    )"
      ],
      "metadata": {
        "id": "6pijIAc8Htww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_chileno.predict(\n",
        "    input=\"Quiero comprar un auto, ya!\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Dy11LvrUIIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary_chileno.predict(\n",
        "    input=\"Quiero comprar un auto, ya!\"\n",
        "    )"
      ],
      "metadata": {
        "id": "hNU4NQNAeSXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Entity memory - Guarda las entidades clave en la memoria\n",
        "\n",
        "La memoria de entidades se comporta de manera diferente a los tipos anteriores. Esta estrategia de memoria se centra en reconocer y recordar entidades específicas mencionadas en la conversación. Esto es particularmente útil para chatbots que necesitan extraer y entender información clave, como nombres de personas, identificadores de productos y otros detalles importantes."
      ],
      "metadata": {
        "id": "wsKeMXEkxp9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationEntityMemory\n",
        "from langchain.chains.conversation.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
      ],
      "metadata": {
        "id": "Pm_BKZMTyHE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cualquier momento podemos editar el template ingresado originalmente al modelo. La clave es recordar que es simplemente un objeto de tipo PromptTemplate (nota que el prompt cambia para cada diferente tipo de memoria) pero que tiene diferentes `input_variables` de acuerdo a cómo funciona el estilo de memoria. Es siempre util ver el template que se está utilizando para ver estas `input_variables` antes de crear nuestro prompio prompt template para este estilo de memoria.\n",
        "\n",
        "Por ejemplo para la Entity Memory Conversation tenemos que usar tres input_variables: input_variables=['entities', 'history', 'input']"
      ],
      "metadata": {
        "id": "ImNTiprPzAcA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VC7hKaPvyJ2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EEojAMH06hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí creamos nuestro propio prompt con un poco de sabor Chilango (persona nacida en la Ciudad de México)."
      ],
      "metadata": {
        "id": "UQUxfouD2BlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Eres un asistente de ventas para una empresa de máquinas de micheladas,\n",
        "\n",
        "Solo estás diseñado para (1) buscar resolver el problema con la máquina de micheladas, y si el cliente no lo logra, (2) agendar la visita de un técnico especializado.\n",
        "\n",
        "Si el cliente no ha logrado arreglar la máquina, entonces pregunta si quiere agendar una visita con el técnico. Si el cliente quiere agendar una visita con el técnico entonces debe dejar su número de celular y dirección.\n",
        "\n",
        "También cuentas con acceso a información personalizada proporcionada por el humano en la sección de Contexto a continuación.\n",
        "\n",
        "Estás aquí para ayudar, siempre con la chispa y el carácter de alguien nacido en Tepito, México.\n",
        "\n",
        "Es clave que preguntes la fecha en qué compraron la máquina, su número de garantía y quién los atendió. Pregunta siempre si se logro resolver el problema o duda del cliente.\n",
        "\n",
        "Contexto:\n",
        "{entities}\n",
        "\n",
        "Conversación actual:\n",
        "{history}\n",
        "\n",
        "Última línea:\n",
        "Humano: {input}\n",
        "\n",
        "Tú (nacido en Tepito):\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "27fmwHqPzhCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model_name='text-davinci-003',\n",
        "             temperature=0,\n",
        "             max_tokens = 256)"
      ],
      "metadata": {
        "id": "Gil7TugkyT5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7R0eNA2yWA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos la conversación:"
      ],
      "metadata": {
        "id": "2RjZlgJXxxkm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ywlp4muA2eef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzGuIRRe2w23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "152J3BiY4pPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n"
      ],
      "metadata": {
        "id": "dvLLSnGM69AV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}