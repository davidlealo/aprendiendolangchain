{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC0rFwuDqtMN"
   },
   "source": [
    "# Indexes o √≠ndices\n",
    "\n",
    "Los √≠ndices se refieren a las formas de estructurar documentos para que los Modelos de Lenguaje de Masivos (LLMs) puedan interactuar con ellos de la mejor manera posible. Esta es una tarea esencial para optimizar la eficiencia y velocidad de las operaciones de b√∫squeda y recuperaci√≥n de informaci√≥n en sistemas de procesamiento de lenguaje natural.\n",
    "\n",
    "Puedes pensar en los √≠ndices como en el √≠ndice de un libro. En un libro el √≠ndice te ayuda a localizar r√°pidamente un cap√≠tulo o secci√≥n espec√≠fica sin tener que hojear todas las p√°ginas. De manera similar, los √≠ndices en LangChain permiten a los LLMs encontrar r√°pidamente documentos o informaci√≥n relevantes sin tener que procesar todos los documentos disponibles.\n",
    "\n",
    "## √çndices y recuperaci√≥n\n",
    "\n",
    "El uso m√°s com√∫n de los √≠ndices en las cadenas de procesamiento de datos es en un paso denominado **\"recuperaci√≥n\"**. Este paso se refiere a tomar la consulta de un usuario y devolver los documentos m√°s relevantes. Sin embargo, es importante hacer una distinci√≥n aqu√≠ porque:\n",
    "\n",
    "1. Un √≠ndice puede utilizarse para otras cosas adem√°s de la recuperaci√≥n.\n",
    "2. La recuperaci√≥n puede utilizar otras l√≥gicas adem√°s de un √≠ndice para encontrar documentos relevantes.\n",
    "\n",
    "La mayor√≠a de las veces, cuando hablamos de √≠ndices y recuperaci√≥n, nos referimos a la indexaci√≥n y recuperaci√≥n de datos no estructurados, como documentos de texto. En este contexto, \"no estructurado\" significa que los datos no siguen un formato fijo o predecible, como lo hace, por ejemplo, una tabla de base de datos. En cambio, los documentos de texto pueden variar ampliamente en t√©rminos de longitud, estilo, contenido, etc.\n",
    "\n",
    "## Retriever en LangChain\n",
    "\n",
    "El **Retriever** es un componente fundamental en el ecosistema de LangChain. Su responsabilidad principal es localizar y devolver documentos relevantes seg√∫n una consulta espec√≠fica. Imag√≠nate un bibliotecario diligente que sabe exactamente d√≥nde encontrar el libro que necesitas en una gran biblioteca; eso es lo que hace el Retriever en LangChain.\n",
    "\n",
    "Para realizar esta tarea, el Retriever debe implementar el m√©todo `get_relevant_documents`. Aunque este m√©todo puede ser implementado de la forma que el usuario considere m√°s conveniente, en LangChain se ha dise√±ado una estrategia para recuperar documentos lo m√°s eficientemente posible. Esta estrategia se basa en el concepto de **Vectorstore**, por lo que vamos a centrarnos en el Retriever tipo Vectorstore en el resto de esta gu√≠a.\n",
    "\n",
    "### Vectorstore y Vectorstore Retriever\n",
    "\n",
    "Para entender qu√© es un **Retriever** tipo **Vectorstore**, primero debemos entender qu√© es un Vectorstore. Un Vectorstore es un tipo de base de datos especialmente dise√±ada para gestionar y manipular vectores de alta dimensionalidad, com√∫nmente utilizados para representar datos en aprendizaje autom√°tico y otras aplicaciones de inteligencia artificial.\n",
    "\n",
    "En la analog√≠a de la biblioteca mencionada anteriormente, si el Retriever es el bibliotecario, entonces el Vectorstore ser√≠a el sistema de clasificaci√≥n y organizaci√≥n de la biblioteca que permite al bibliotecario encontrar exactamente lo que busca.\n",
    "\n",
    "En LangChain, el sistema Vectorstore predeterminado que se utiliza es Chroma. Chroma se utiliza para indexar y buscar embeddings (vectores que representan documentos en el espacio multidimensional). Estos embeddings son una forma de condensar y representar la informaci√≥n de un documento para que pueda ser f√°cilmente comparable con otros documentos.\n",
    "\n",
    "El Retriever tipo Vectorstore, por lo tanto, es un tipo de Retriever que utiliza una base de datos Vectorstore (como Chroma) para localizar documentos relevantes para una consulta espec√≠fica. Primero transforma la consulta en un vector (a trav√©s de un proceso de incrustaci√≥n (embedding)), luego busca en la base de datos Vectorstore los documentos cuyos vectores son m√°s cercanos (en t√©rminos de distancia coseno u otras m√©tricas de similitud) a la consulta vectorizada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UlvsHbePQe8"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hwZ6Qw6cOcfi",
    "outputId": "e7fbd3ec-c972-4574-8e03-1a35b136e4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.216\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langchainplus-sdk, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jcA9fV-P4ak"
   },
   "source": [
    "## 1. La clase Document\n",
    "\n",
    "Esta clase es la base de cuando carguemos nuestros documentos. En LangChain se les llama schemas a estas clases base y se encuentran en langchain.schema. As√≠ es el schema para Document:\n",
    "\n",
    "```\n",
    "class Document(Serializable):\n",
    "    \"\"\"Interface for interacting with a document.\"\"\"\n",
    "\n",
    "    page_content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FFQE9GQP7dn"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "page_content = \"Textooooooooolargoooooo ejemplo\"\n",
    "metadata = {'fuente': 'platzi', 'clase': 'langchain'}\n",
    "\n",
    "doc = Document(\n",
    "    page_content=page_content, metadata=metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h8HsrNf8P-7x",
    "outputId": "433c78d4-d5b0-4d97-e26c-24c3270cfa90"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Textooooooooolargoooooo ejemplo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8K06AJXuY5D"
   },
   "source": [
    "## 2. Document loaders\n",
    "\n",
    "La primera etapa en la indexaci√≥n de documentos en LangChain implica cargar los datos en \"Documentos\". Este es el nombre de la clase con la que trabajaremos, ubicada en el directorio de esquemas en el repositorio de LangChain. Simplificando, un \"Documento\" es b√°sicamente un fragmento de texto. El prop√≥sito del cargador de documentos es simplificar este proceso de carga.\n",
    "\n",
    "### Document transformers\n",
    "\n",
    "Los transformadores de carga son utilidades que convierten los datos desde un formato espec√≠fico al formato \"Documento\". Por ejemplo, existen transformadores para los formatos CSV y SQL. En su mayor√≠a, estos cargadores obtienen datos de archivos, pero a veces tambi√©n de URLs.\n",
    "\n",
    "Existen varios cargadores de documentos dependiendo de la fuente de nuestros datos. A continuaci√≥n, se muestran algunos ejemplos (para m√°s informaci√≥n, consulta la documentaci√≥n):\n",
    "\n",
    "- Airtable\n",
    "- OpenAIWhisperParser\n",
    "- CoNLL-U\n",
    "- Copy Paste\n",
    "- CSV\n",
    "- Email\n",
    "- EPub\n",
    "- EverNote\n",
    "- Microsoft Excel\n",
    "- Facebook Chat\n",
    "- File Directory\n",
    "- HTML\n",
    "- Images\n",
    "- Jupyter Notebook\n",
    "- JSON\n",
    "- Markdown\n",
    "- Microsoft PowerPoint\n",
    "- Microsoft Word\n",
    "- Open Document Format (ODT)\n",
    "- Pandas DataFrame\n",
    "- PDF\n",
    "\n",
    "Al mismo tiempo, tambi√©n puedes utilizar servicios como los datasets de Hugging Face, o incluso obtener datos de servicios como Slack, Snowflake, Spreedly, Stripe, 2Markdown, entre otros.\n",
    "\n",
    "Cada mes se a√±aden nuevas fuentes y tipos de conjuntos de datos que podemos utilizar. Te recomendamos revisar la documentaci√≥n con regularidad para mantenerte actualizado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L7tqqde1P0m"
   },
   "source": [
    "Comencemos con ejemplos, usemos un paper descargado de internet y de alta relevancia para nuestras vidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74j2GuJaS67h"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.cs.virginia.edu/~evans/greatworks/diffie.pdf'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('public_key_cryptography.pdf', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkhiucXQ076o"
   },
   "source": [
    "Quiz√°s el Document Loader m√°s relevante es el unstructured pues se encuentra como la base de otros Document Loaders. Sirve por ejemplo para documentos de texto como .txt o .pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVWuupfm0UIa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGQJV_C7zogx",
    "outputId": "0f5a8e6f-7839-4507-a966-8ad102b34f99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./public_key_cryptography.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "4iKCY0_nTd3h",
    "outputId": "fcee93ff-8636-43c0-9c7e-24f0bb42387e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The First Ten Years of Public-Key Cryptography\\n\\nWH lTFl ELD DI FFlE\\n\\nInvited Paper\\n\\nPublic-key cryptosystems separate the capacities for encryption and decryption so that 7) many people can encrypt messages in such a way that only one person can read them, or 2) one person can encrypt messages in su'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4pYWttM0fdg"
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaV1WK3AMxHh"
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIDJWm_Z1rKv"
   },
   "outputs": [],
   "source": [
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zw5KUS-UsGH"
   },
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR3FpP5l0wUJ"
   },
   "source": [
    "Existen alternativas que mantienen las p√°ginas del documento PDF en caso de ser necesario esto. Probablemente el m√°s usado es usando PyPDFLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2lqqcV3yKnU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6E5-5rpxyYr"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./public_key_cryptography.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0foD62tUOsj",
    "outputId": "67e84be8-6c26-4b3f-d05c-6aa37d379c2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './public_key_cryptography.pdf', 'page': 17}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[17].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLKdKyZYyV9D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_MtTDBLNy3O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kWoQK0x3lSR"
   },
   "source": [
    "Otro uso frecuente es leer datos de CSVs o Spreadsheets (como Excel), muchas empresas tienen sus datos en este formato. Primero, debemos tener el archivo en formato de un DataFrame de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_hguO3IHbXa"
   },
   "source": [
    "### CSV a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBWa5MX037OW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weRvWvu23-wu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('repos_cairo.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq6Vz4CY3rDY"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"repo_name\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mV0RHdLJ4U6Y"
   },
   "outputs": [],
   "source": [
    "print(f\"El archivo es de tipo {type(data)} y tiene una longitud de {len(data)} debido a la cantidad de observaciones en el CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9l7VTap542Z3"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(data[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtJSg7XlHTYs"
   },
   "source": [
    "### JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp549bV85Qj7"
   },
   "source": [
    "Veamos un caso m√°s complejo. No tenemos una implementaci√≥n directa de LangChain para importar **JSONLs** sin embargo es muy com√∫n tener que importar estos formatos.\n",
    "\n",
    "El siguiente ejemplo muestra c√≥mo importar un JSONL personalizado para nuestra base de datos de Transformers, pero aplica para otros formatos de datos que no necesariamente se encuentran entre los disponibles por LangChain. Nosotros creamos nuestros Document seg√∫n lo que queramos asignar como page_content y metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VgsvL7SmG6E"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "henKq3AdKasX"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import jsonlines\n",
    "from typing import List\n",
    "\n",
    "class TransformerDocsJSONLLoader:\n",
    "  def __init__(self, file_path: str):\n",
    "    self.file_path = file_path\n",
    "\n",
    "  def load(self):\n",
    "    with jsonlines.open(self.file_path) as reader:\n",
    "      documents = []\n",
    "      for obj in reader:\n",
    "        page_content = obj.get(\"text\", \"\")\n",
    "        metadata = {\n",
    "            'title': obj.get(\"title\", \"\"),\n",
    "            'repo_owner' : obj.get(\"repo_owner\", \"\"),\n",
    "            'repo_name' : obj.get(\"repo_name\", \"\"),\n",
    "        }\n",
    "        documents.append(\n",
    "            Document(page_content=page_content, metadata=metadata)\n",
    "        )\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8xyKNzRq0H_"
   },
   "outputs": [],
   "source": [
    "loader = TransformerDocsJSONLLoader(\"transformers_docs.jsonl\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvrjRorTrEZH",
    "outputId": "7439dd63-9a1b-4742-801b-976baf365143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='<!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Distributed training with ü§ó Accelerate\\n\\nAs models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the [ü§ó Accelerate](https://huggingface.co/docs/accelerate) library to help users easily train a ü§ó Transformers model on any type of distributed setup, whether it is multiple GPU\\'s on one machine or multiple GPU\\'s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.\\n\\n## Setup\\n\\nGet started by installing ü§ó Accelerate:\\n\\n```bash\\npip install accelerate\\n```\\n\\nThen import and create an [`~accelerate.Accelerator`] object. The [`~accelerate.Accelerator`] will automatically detect your type of distributed setup and initialize all the necessary components for training. You don\\'t need to explicitly place your model on a device.\\n\\n```py\\n>>> from accelerate import Accelerator\\n\\n>>> accelerator = Accelerator()\\n```\\n\\n## Prepare to accelerate\\n\\nThe next step is to pass all the relevant training objects to the [`~accelerate.Accelerator.prepare`] method. This includes your training and evaluation DataLoaders, a model and an optimizer:\\n\\n```py\\n>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\\n...     train_dataloader, eval_dataloader, model, optimizer\\n... )\\n```\\n\\n## Backward\\n\\nThe last addition is to replace the typical `loss.backward()` in your training loop with ü§ó Accelerate\\'s [`~accelerate.Accelerator.backward`]method:\\n\\n```py\\n>>> for epoch in range(num_epochs):\\n...     for batch in train_dataloader:\\n...         outputs = model(**batch)\\n...         loss = outputs.loss\\n...         accelerator.backward(loss)\\n\\n...         optimizer.step()\\n...         lr_scheduler.step()\\n...         optimizer.zero_grad()\\n...         progress_bar.update(1)\\n```\\n\\nAs you can see in the following code, you only need to add four additional lines of code to your training loop to enable distributed training!\\n\\n```diff\\n+ from accelerate import Accelerator\\n  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\\n\\n+ accelerator = Accelerator()\\n\\n  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\\n  optimizer = AdamW(model.parameters(), lr=3e-5)\\n\\n- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n- model.to(device)\\n\\n+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\\n+     train_dataloader, eval_dataloader, model, optimizer\\n+ )\\n\\n  num_epochs = 3\\n  num_training_steps = num_epochs * len(train_dataloader)\\n  lr_scheduler = get_scheduler(\\n      \"linear\",\\n      optimizer=optimizer,\\n      num_warmup_steps=0,\\n      num_training_steps=num_training_steps\\n  )\\n\\n  progress_bar = tqdm(range(num_training_steps))\\n\\n  model.train()\\n  for epoch in range(num_epochs):\\n      for batch in train_dataloader:\\n-         batch = {k: v.to(device) for k, v in batch.items()}\\n          outputs = model(**batch)\\n          loss = outputs.loss\\n-         loss.backward()\\n+         accelerator.backward(loss)\\n\\n          optimizer.step()\\n          lr_scheduler.step()\\n          optimizer.zero_grad()\\n          progress_bar.update(1)\\n```\\n\\n## Train\\n\\nOnce you\\'ve added the relevant lines of code, launch your training in a script or a notebook like Colaboratory.\\n\\n### Train with a script\\n\\nIf you are running your training from a script, run the following command to create and save a configuration file:\\n\\n```bash\\naccelerate config\\n```\\n\\nThen launch your training with:\\n\\n```bash\\naccelerate launch train.py\\n```\\n\\n### Train with a notebook\\n\\nü§ó Accelerate can also run in a notebook if you\\'re planning on using Colaboratory\\'s TPUs. Wrap all the code responsible for training in a function, and pass it to [`~accelerate.notebook_launcher`]:\\n\\n```py\\n>>> from accelerate import notebook_launcher\\n\\n>>> notebook_launcher(training_function)\\n```\\n\\nFor more information about ü§ó Accelerate and it\\'s rich features, refer to the [documentation](https://huggingface.co/docs/accelerate).' metadata={'title': 'accelerate.mdx', 'repo_owner': 'huggingface', 'repo_name': 'transformers'}\n",
      "page_content='<!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n-->\\n\\n# How to add a model to ü§ó Transformers?\\n\\nThe ü§ó Transformers library is often able to offer new models thanks to community contributors. But this can be a challenging project and requires an in-depth knowledge of the ü§ó Transformers library and the model to implement. At Hugging Face, we\\'re trying to empower more of the community to actively add models and we\\'ve put together this guide to walk you through the process of adding a PyTorch model (make sure you have [PyTorch installed](https://pytorch.org/get-started/locally/)).\\n\\n<Tip>\\n\\nIf you\\'re interested in implementing a TensorFlow model, take a look at the [How to convert a ü§ó Transformers model to TensorFlow](add_tensorflow_model) guide!\\n\\n</Tip>\\n\\nAlong the way, you\\'ll:\\n\\n- get insights into open-source best practices\\n- understand the design principles behind one of the most popular deep learning libraries\\n- learn how to efficiently test large models\\n- learn how to integrate Python utilities like `black`, `ruff`, and `make fix-copies` to ensure clean and readable code\\n\\nA Hugging Face team member will be available to help you along the way so you\\'ll never be alone. ü§ó ‚ù§Ô∏è\\n\\nTo get started, open a [New model addition](https://github.com/huggingface/transformers/issues/new?assignees=&labels=New+model&template=new-model-addition.yml) issue for the model you want to see in ü§ó Transformers. If you\\'re not especially picky about contributing a specific model, you can filter by the [New model label](https://github.com/huggingface/transformers/labels/New%20model) to see if there are any unclaimed model requests and work on it.\\n\\nOnce you\\'ve opened a new model request, the first step is to get familiar with ü§ó Transformers if you aren\\'t already!\\n\\n## General overview of ü§ó Transformers\\n\\nFirst, you should get a general overview of ü§ó Transformers. ü§ó Transformers is a very opinionated library, so there is a\\nchance that you don\\'t agree with some of the library\\'s philosophies or design choices. From our experience, however, we\\nfound that the fundamental design choices and philosophies of the library are crucial to efficiently scale ü§ó\\nTransformers while keeping maintenance costs at a reasonable level.\\n\\nA good first starting point to better understand the library is to read the [documentation of our philosophy](philosophy). As a result of our way of working, there are some choices that we try to apply to all models:\\n\\n- Composition is generally favored over-abstraction\\n- Duplicating code is not always bad if it strongly improves the readability or accessibility of a model\\n- Model files are as self-contained as possible so that when you read the code of a specific model, you ideally only\\n  have to look into the respective `modeling_....py` file.\\n\\nIn our opinion, the library\\'s code is not just a means to provide a product, *e.g.* the ability to use BERT for\\ninference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the\\nperson that will use your model, but also everybody that will read, try to understand, and possibly tweak your code.\\n\\nWith this in mind, let\\'s go a bit deeper into the general library design.\\n\\n### Overview of models\\n\\nTo successfully add a model, it is important to understand the interaction between your model and its config,\\n[`PreTrainedModel`], and [`PretrainedConfig`]. For exemplary purposes, we will\\ncall the model to be added to ü§ó Transformers `BrandNewBert`.\\n\\nLet\\'s take a look:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png\"/>\\n\\nAs you can see, we do make use of inheritance in ü§ó Transformers, but we keep the level of abstraction to an absolute\\nminimum. There are never more than two levels of abstraction for any model in the library. `BrandNewBertModel`\\ninherits from `BrandNewBertPreTrainedModel` which in turn inherits from [`PreTrainedModel`] and\\nthat\\'s it. As a general rule, we want to make sure that a new model only depends on\\n[`PreTrainedModel`]. The important functionalities that are automatically provided to every new\\nmodel are [`~PreTrainedModel.from_pretrained`] and\\n[`~PreTrainedModel.save_pretrained`], which are used for serialization and deserialization. All of the\\nother important functionalities, such as `BrandNewBertModel.forward` should be completely defined in the new\\n`modeling_brand_new_bert.py` script. Next, we want to make sure that a model with a specific head layer, such as\\n`BrandNewBertForMaskedLM` does not inherit from `BrandNewBertModel`, but rather uses `BrandNewBertModel`\\nas a component that can be called in its forward pass to keep the level of abstraction low. Every new model requires a\\nconfiguration class, called `BrandNewBertConfig`. This configuration is always stored as an attribute in\\n[`PreTrainedModel`], and thus can be accessed via the `config` attribute for all classes\\ninheriting from `BrandNewBertPreTrainedModel`:\\n\\n```python\\nmodel = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\\nmodel.config  # model has access to its config\\n```\\n\\nSimilar to the model, the configuration inherits basic serialization and deserialization functionalities from\\n[`PretrainedConfig`]. Note that the configuration and the model are always serialized into two\\ndifferent formats - the model to a *pytorch_model.bin* file and the configuration to a *config.json* file. Calling\\n[`~PreTrainedModel.save_pretrained`] will automatically call\\n[`~PretrainedConfig.save_pretrained`], so that both model and configuration are saved.\\n\\n\\n### Code style\\n\\nWhen coding your new model, keep in mind that Transformers is an opinionated library and we have a few quirks of our\\nown regarding how code should be written :-)\\n\\n1. The forward pass of your model should be fully written in the modeling file while being fully independent of other\\n   models in the library. If you want to reuse a block from another model, copy the code and paste it with a\\n   `# Copied from` comment on top (see [here](https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160)\\n   for a good example).\\n2. The code should be fully understandable, even by a non-native English speaker. This means you should pick\\n   descriptive variable names and avoid abbreviations. As an example, `activation` is preferred to `act`.\\n   One-letter variable names are strongly discouraged unless it\\'s an index in a for loop.\\n3. More generally we prefer longer explicit code to short magical one.\\n4. Avoid subclassing `nn.Sequential` in PyTorch but subclass `nn.Module` and write the forward pass, so that anyone\\n   using your code can quickly debug it by adding print statements or breaking points.\\n5. Your function signature should be type-annotated. For the rest, good variable names are way more readable and\\n   understandable than type annotations.\\n\\n### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\n## Step-by-step recipe to add a model to ü§ó Transformers\\n\\nEveryone has different preferences of how to port a model so it can be very helpful for you to take a look at summaries\\nof how other contributors ported models to Hugging Face. Here is a list of community blog posts on how to port a model:\\n\\n1. [Porting GPT2 Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28) by [Thomas](https://huggingface.co/thomwolf)\\n2. [Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt) by [Stas](https://huggingface.co/stas)\\n\\nFrom experience, we can tell you that the most important things to keep in mind when adding a model are:\\n\\n-  Don\\'t reinvent the wheel! Most parts of the code you will add for the new ü§ó Transformers model already exist\\n  somewhere in ü§ó Transformers. Take some time to find similar, already existing models and tokenizers you can copy\\n  from. [grep](https://www.gnu.org/software/grep/) and [rg](https://github.com/BurntSushi/ripgrep) are your\\n  friends. Note that it might very well happen that your model\\'s tokenizer is based on one model implementation, and\\n  your model\\'s modeling code on another one. *E.g.* FSMT\\'s modeling code is based on BART, while FSMT\\'s tokenizer code\\n  is based on XLM.\\n-  It\\'s more of an engineering challenge than a scientific challenge. You should spend more time on creating an\\n  efficient debugging environment than trying to understand all theoretical aspects of the model in the paper.\\n-  Ask for help, when you\\'re stuck! Models are the core component of ü§ó Transformers so that we at Hugging Face are more\\n  than happy to help you at every step to add your model. Don\\'t hesitate to ask if you notice you are not making\\n  progress.\\n\\nIn the following, we try to give you a general recipe that we found most useful when porting a model to ü§ó Transformers.\\n\\nThe following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do\\nList:\\n\\n‚òê (Optional) Understood the model\\'s theoretical aspects<br>\\n‚òê Prepared ü§ó Transformers dev environment<br>\\n‚òê Set up debugging environment of the original repository<br>\\n‚òê Created script that successfully runs the `forward()` pass using the original repository and checkpoint<br>\\n‚òê Successfully added the model skeleton to ü§ó Transformers<br>\\n‚òê Successfully converted original checkpoint to ü§ó Transformers checkpoint<br>\\n‚òê Successfully ran `forward()` pass in ü§ó Transformers that gives identical output to original checkpoint<br>\\n‚òê Finished model tests in ü§ó Transformers<br>\\n‚òê Successfully added tokenizer in ü§ó Transformers<br>\\n‚òê Run end-to-end integration tests<br>\\n‚òê Finished docs<br>\\n‚òê Uploaded model weights to the Hub<br>\\n‚òê Submitted the pull request<br>\\n‚òê (Optional) Added a demo notebook\\n\\nTo begin with, we usually recommend to start by getting a good theoretical understanding of `BrandNewBert`. However,\\nif you prefer to understand the theoretical aspects of the model *on-the-job*, then it is totally fine to directly dive\\ninto the `BrandNewBert`\\'s code-base. This option might suit you better, if your engineering skills are better than\\nyour theoretical skill, if you have trouble understanding `BrandNewBert`\\'s paper, or if you just enjoy programming\\nmuch more than reading scientific papers.\\n\\n### 1. (Optional) Theoretical aspects of BrandNewBert\\n\\nYou should take some time to read *BrandNewBert\\'s* paper, if such descriptive work exists. There might be large\\nsections of the paper that are difficult to understand. If this is the case, this is fine - don\\'t worry! The goal is\\nnot to get a deep theoretical understanding of the paper, but to extract the necessary information required to\\neffectively re-implement the model in ü§ó Transformers. That being said, you don\\'t have to spend too much time on the\\ntheoretical aspects, but rather focus on the practical ones, namely:\\n\\n-  What type of model is *brand_new_bert*? BERT-like encoder-only model? GPT2-like decoder-only model? BART-like\\n  encoder-decoder model? Look at the [model_summary](model_summary) if you\\'re not familiar with the differences between those.\\n-  What are the applications of *brand_new_bert*? Text classification? Text generation? Seq2Seq tasks, *e.g.,*\\n  summarization?\\n-  What is the novel feature of the model making it different from BERT/GPT-2/BART?\\n-  Which of the already existing [ü§ó Transformers models](https://huggingface.co/transformers/#contents) is most\\n  similar to *brand_new_bert*?\\n-  What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as used\\n  for BERT or BART?\\n\\nAfter you feel like you have gotten a good overview of the architecture of the model, you might want to write to the\\nHugging Face team with any questions you might have. This might include questions regarding the model\\'s architecture,\\nits attention layer, etc. We will be more than happy to help you.\\n\\n### 2. Next prepare your environment\\n\\n1. Fork the [repository](https://github.com/huggingface/transformers) by clicking on the ‚ÄòFork\\' button on the\\n   repository\\'s page. This creates a copy of the code under your GitHub user account.\\n\\n2. Clone your `transformers` fork to your local disk, and add the base repository as a remote:\\n\\n```bash\\ngit clone https://github.com/[your Github handle]/transformers.git\\ncd transformers\\ngit remote add upstream https://github.com/huggingface/transformers.git\\n```\\n\\n3. Set up a development environment, for instance by running the following command:\\n\\n```bash\\npython -m venv .env\\nsource .env/bin/activate\\npip install -e \".[dev]\"\\n```\\n\\nDepending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\\nfailure with this command. If that\\'s the case make sure to install the Deep Learning framework you are working with\\n(PyTorch, TensorFlow and/or Flax) then do:\\n\\n```bash\\npip install -e \".[quality]\"\\n```\\n\\nwhich should be enough for most use cases. You can then return to the parent directory\\n\\n```bash\\ncd ..\\n```\\n\\n4. We recommend adding the PyTorch version of *brand_new_bert* to Transformers. To install PyTorch, please follow the\\n   instructions on https://pytorch.org/get-started/locally/.\\n\\n**Note:** You don\\'t need to have CUDA installed. Making the new model work on CPU is sufficient.\\n\\n5. To port *brand_new_bert*, you will also need access to its original repository:\\n\\n```bash\\ngit clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git\\ncd brand_new_bert\\npip install -e .\\n```\\n\\nNow you have set up a development environment to port *brand_new_bert* to ü§ó Transformers.\\n\\n### 3.-4. Run a pretrained checkpoint using the original repository\\n\\nAt first, you will work on the original *brand_new_bert* repository. Often, the original implementation is very\\n‚Äúresearchy‚Äù. Meaning that documentation might be lacking and the code can be difficult to understand. But this should\\nbe exactly your motivation to reimplement *brand_new_bert*. At Hugging Face, one of our main goals is to *make people\\nstand on the shoulders of giants* which translates here very well into taking a working model and rewriting it to make\\nit as **accessible, user-friendly, and beautiful** as possible. This is the number-one motivation to re-implement\\nmodels into ü§ó Transformers - trying to make complex new NLP technology accessible to **everybody**.\\n\\nYou should start thereby by diving into the original repository.\\n\\nSuccessfully running the official pretrained model in the original repository is often **the most difficult** step.\\nFrom our experience, it is very important to spend some time getting familiar with the original code-base. You need to\\nfigure out the following:\\n\\n- Where to find the pretrained weights?\\n- How to load the pretrained weights into the corresponding model?\\n- How to run the tokenizer independently from the model?\\n- Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,\\n  you only have to reimplement those functions.\\n- Be able to locate the important components of the model: Where is the model\\'s class? Are there model sub-classes,\\n  *e.g.* EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,\\n  *e.g.* *self-attention*, *cross-attention*...?\\n- How can you debug the model in the original environment of the repo? Do you have to add *print* statements, can you\\n  work with an interactive debugger like *ipdb*, or should you use an efficient IDE to debug the model, like PyCharm?\\n\\nIt is very important that before you start the porting process, that you can **efficiently** debug code in the original\\nrepository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or\\neven a pull request in the original repository. The maintainers of this repository are most likely very happy about\\nsomeone looking into their code!\\n\\nAt this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original\\nmodel. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to\\ndive into the original repository and also when starting to write the ü§ó Transformers implementation of the model. Only\\nat the very end, when the model has already been successfully ported to ü§ó Transformers, one should verify that the\\nmodel also works as expected on GPU.\\n\\nIn general, there are two possible debugging environments for running the original model\\n\\n-  [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\\n-  Local python scripts.\\n\\nJupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split\\nlogical components from one another and to have faster debugging cycles as intermediate results can be stored. Also,\\nnotebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging\\nFace team for help. If you are familiar with Jupyter notebooks, we strongly recommend you to work with them.\\n\\nThe obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend\\nsome time adjusting to the new programming environment and that you might not be able to use your known debugging tools\\nanymore, like `ipdb`.\\n\\nFor each code-base, a good first step is always to load a **small** pretrained checkpoint and to be able to reproduce a\\nsingle forward pass using a dummy integer vector of input IDs as an input. Such a script could look like this (in\\npseudocode):\\n\\n```python\\nmodel = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\\noriginal_output = model.predict(input_ids)\\n```\\n\\nNext, regarding the debugging strategy, there are generally a few from which to choose from:\\n\\n- Decompose the original model into many small testable components and run a forward pass on each of those for\\n  verification\\n- Decompose the original model only into the original *tokenizer* and the original *model*, run a forward pass on\\n  those, and use intermediate print statements or breakpoints for verification\\n\\nAgain, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code\\nbase.\\n\\nIf the original code-base allows you to decompose the model into smaller sub-components, *e.g.* if the original\\ncode-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages\\nto taking the more difficult road in the beginning:\\n\\n- at a later stage when comparing the original model to the Hugging Face implementation, you can verify automatically\\n  for each component individually that the corresponding component of the ü§ó Transformers implementation matches instead\\n  of relying on visual comparison via print statements\\n- it can give you some rope to decompose the big problem of porting a model into smaller problems of just porting\\n  individual components and thus structure your work better\\n- separating the model into logical meaningful components will help you to get a better overview of the model\\'s design\\n  and thus to better understand the model\\n- at a later stage those component-by-component tests help you to ensure that no regression occurs as you continue\\n  changing your code\\n\\n[Lysandre\\'s](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed) integration checks for ELECTRA\\ngives a nice example of how this can be done.\\n\\nHowever, if the original code-base is very complex or only allows intermediate components to be run in a compiled mode,\\nit might be too time-consuming or even impossible to separate the model into smaller testable sub-components. A good\\nexample is [T5\\'s MeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow) library which is\\nvery complex and does not offer a simple way to decompose the model into its sub-components. For such libraries, one\\noften relies on verifying print statements.\\n\\nNo matter which strategy you choose, the recommended procedure is often the same in that you should start to debug the\\nstarting layers first and the ending layers last.\\n\\nIt is recommended that you retrieve the output, either by print statements or sub-component functions, of the following\\nlayers in the following order:\\n\\n1. Retrieve the input IDs passed to the model\\n2. Retrieve the word embeddings\\n3. Retrieve the input of the first Transformer layer\\n4. Retrieve the output of the first Transformer layer\\n5. Retrieve the output of the following n - 1 Transformer layers\\n6. Retrieve the output of the whole BrandNewBert Model\\n\\nInput IDs should thereby consists of an array of integers, *e.g.* `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`\\n\\nThe outputs of the following layers often consist of multi-dimensional float arrays and can look like this:\\n\\n```\\n[[\\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\\n [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\\n [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\\n ...,\\n [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\\n [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\\n [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],\\n```\\n\\nWe expect that every model added to ü§ó Transformers passes a couple of integration tests, meaning that the original\\nmodel and the reimplemented version in ü§ó Transformers have to give the exact same output up to a precision of 0.001!\\nSince it is normal that the exact same model written in different libraries can give a slightly different output\\ndepending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives\\nnearly the same output, they have to be the almost identical. Therefore, you will certainly compare the intermediate\\noutputs of the ü§ó Transformers version multiple times against the intermediate outputs of the original implementation of\\n*brand_new_bert* in which case an **efficient** debugging environment of the original repository is absolutely\\nimportant. Here is some advice is to make your debugging environment as efficient as possible.\\n\\n- Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should\\n  probably take the time to write a longer script that decomposes the original model into smaller sub-components to\\n  retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on\\n  TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to output\\n  intermediate values. Is the original repository written in Jax? Then make sure that the model is **not jitted** when\\n  running the forward pass, *e.g.* check-out [this link](https://github.com/google/jax/issues/196).\\n- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle\\n  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.\\n  In case only very large checkpoints are available, it might make more sense to create a dummy model in the new\\n  environment with randomly initialized weights and save those weights for comparison with the ü§ó Transformers version\\n  of your model\\n- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to\\n  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often called\\n  `predict`, `evaluate`, `forward` or `__call__`. You don\\'t want to debug a function that calls `forward`\\n  multiple times, *e.g.* to generate text, like `autoregressive_sample`, `generate`.\\n- Try to separate the tokenization from the model\\'s *forward* pass. If the original repository shows examples where\\n  you have to input a string, then try to find out where in the forward call the string input is changed to input ids\\n  and start from this point. This might mean that you have to possibly write a small script yourself or change the\\n  original code so that you can directly input the ids instead of an input string.\\n- Make sure that the model in your debugging setup is **not** in training mode, which often causes the model to yield\\n  random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging\\n  environment is **deterministic** so that the dropout layers are not used. Or use *transformers.utils.set_seed*\\n  if the old and new implementations are in the same framework.\\n\\nThe following section gives you more specific details/tips on how you can do this for *brand_new_bert*.\\n\\n### 5.-14. Port BrandNewBert to ü§ó Transformers\\n\\nNext, you can finally start adding new code to ü§ó Transformers. Go into the clone of your ü§ó Transformers\\' fork:\\n\\n```bash\\ncd transformers\\n```\\n\\nIn the special case that you are adding a model whose architecture exactly matches the model architecture of an\\nexisting model you only have to add a conversion script as described in [this section](#write-a-conversion-script).\\nIn this case, you can just re-use the whole model architecture of the already existing model.\\n\\nOtherwise, let\\'s start generating a new model. You have two choices here:\\n\\n- `transformers-cli add-new-model-like` to add a new model like an existing one\\n- `transformers-cli add-new-model` to add a new model from our template (will look like BERT or Bart depending on the type of model you select)\\n\\nIn both cases, you will be prompted with a questionnaire to fill the basic information of your model. The second command requires to install `cookiecutter`, you can find more information on it [here](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model).\\n\\n**Open a Pull Request on the main huggingface/transformers repo**\\n\\nBefore starting to adapt the automatically generated code, now is the time to open a ‚ÄúWork in progress (WIP)‚Äù pull\\nrequest, *e.g.* ‚Äú[WIP] Add *brand_new_bert*‚Äù, in ü§ó Transformers so that you and the Hugging Face team can work\\nside-by-side on integrating the model into ü§ó Transformers.\\n\\nYou should do the following:\\n\\n1. Create a branch with a descriptive name from your main branch\\n\\n```bash\\ngit checkout -b add_brand_new_bert\\n```\\n\\n2. Commit the automatically generated code:\\n\\n```bash\\ngit add .\\ngit commit\\n```\\n\\n3. Fetch and rebase to current main\\n\\n```bash\\ngit fetch upstream\\ngit rebase upstream/main\\n```\\n\\n4. Push the changes to your account using:\\n\\n```bash\\ngit push -u origin a-descriptive-name-for-my-changes\\n```\\n\\n5. Once you are satisfied, go to the webpage of your fork on GitHub. Click on ‚ÄúPull request‚Äù. Make sure to add the\\n   GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for\\n   future changes.\\n\\n6. Change the PR into a draft by clicking on ‚ÄúConvert to draft‚Äù on the right of the GitHub pull request web page.\\n\\nIn the following, whenever you have done some progress, don\\'t forget to commit your work and push it to your account so\\nthat it shows in the pull request. Additionally, you should make sure to update your work with the current main from\\ntime to time by doing:\\n\\n```bash\\ngit fetch upstream\\ngit merge upstream/main\\n```\\n\\nIn general, all questions you might have regarding the model or your implementation should be asked in your PR and\\ndiscussed/solved in the PR. This way, the Hugging Face team will always be notified when you are committing new code or\\nif you have a question. It is often very helpful to point the Hugging Face team to your added code so that the Hugging\\nFace team can efficiently understand your problem or question.\\n\\nTo do so, you can go to the ‚ÄúFiles changed‚Äù tab where you see all of your changes, go to a line regarding which you\\nwant to ask a question, and click on the ‚Äú+‚Äù symbol to add a comment. Whenever a question or problem has been solved,\\nyou can click on the ‚ÄúResolve‚Äù button of the created comment.\\n\\nIn the same way, the Hugging Face team will open comments when reviewing your code. We recommend asking most questions\\non GitHub on your PR. For some very general questions that are not very useful for the public, feel free to ping the\\nHugging Face team by Slack or email.\\n\\n**5. Adapt the generated models code for brand_new_bert**\\n\\nAt first, we will focus only on the model itself and not care about the tokenizer. All the relevant code should be\\nfound in the generated files `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` and\\n`src/transformers/models/brand_new_bert/configuration_brand_new_bert.py`.\\n\\nNow you can finally start coding :). The generated code in\\n`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` will either have the same architecture as BERT if\\nit\\'s an encoder-only model or BART if it\\'s an encoder-decoder model. At this point, you should remind yourself what\\nyou\\'ve learned in the beginning about the theoretical aspects of the model: *How is the model different from BERT or\\nBART?*\". Implement those changes which often means to change the *self-attention* layer, the order of the normalization\\nlayer, etc‚Ä¶ Again, it is often useful to look at the similar architecture of already existing models in Transformers to\\nget a better feeling of how your model should be implemented.\\n\\n**Note** that at this point, you don\\'t have to be very sure that your code is fully correct or clean. Rather, it is\\nadvised to add a first *unclean*, copy-pasted version of the original code to\\n`src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` until you feel like all the necessary code is\\nadded. From our experience, it is much more efficient to quickly add a first version of the required code and\\nimprove/correct the code iteratively with the conversion script as described in the next section. The only thing that\\nhas to work at this point is that you can instantiate the ü§ó Transformers implementation of *brand_new_bert*, *i.e.* the\\nfollowing command should work:\\n\\n```python\\nfrom transformers import BrandNewBertModel, BrandNewBertConfig\\n\\nmodel = BrandNewBertModel(BrandNewBertConfig())\\n```\\n\\nThe above command will create a model according to the default parameters as defined in `BrandNewBertConfig()` with\\nrandom weights, thus making sure that the `init()` methods of all components works.\\n\\nNote that all random initialization should happen in the `_init_weights` method of your `BrandnewBertPreTrainedModel`\\nclass. It should initialize all leaf modules depending on the variables of the config. Here is an example with the\\nBERT `_init_weights` method:\\n\\n```py\\ndef _init_weights(self, module):\\n    \"\"\"Initialize the weights\"\"\"\\n    if isinstance(module, nn.Linear):\\n        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\\n        if module.bias is not None:\\n            module.bias.data.zero_()\\n    elif isinstance(module, nn.Embedding):\\n        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\\n        if module.padding_idx is not None:\\n            module.weight.data[module.padding_idx].zero_()\\n    elif isinstance(module, nn.LayerNorm):\\n        module.bias.data.zero_()\\n        module.weight.data.fill_(1.0)\\n```\\n\\nYou can have some more custom schemes if you need a special initialization for some modules. For instance, in\\n`Wav2Vec2ForPreTraining`, the last two linear layers need to have the initialization of the regular PyTorch `nn.Linear`\\nbut all the other ones should use an initialization as above. This is coded like this:\\n\\n```py\\ndef _init_weights(self, module):\\n    \"\"\"Initialize the weights\"\"\"\\n    if isinstnace(module, Wav2Vec2ForPreTraining):\\n        module.project_hid.reset_parameters()\\n        module.project_q.reset_parameters()\\n        module.project_hid._is_hf_initialized = True\\n        module.project_q._is_hf_initialized = True\\n    elif isinstance(module, nn.Linear):\\n        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\\n        if module.bias is not None:\\n            module.bias.data.zero_()\\n```\\n\\nThe `_is_hf_initialized` flag is internally used to make sure we only initialize a submodule once. By setting it to\\n`True` for `module.project_q` and `module.project_hid`, we make sure the custom initialization we did is not overridden later on,\\nthe `_init_weights` function won\\'t be applied to them.\\n\\n**6. Write a conversion script**\\n\\nNext, you should write a conversion script that lets you convert the checkpoint you used to debug *brand_new_bert* in\\nthe original repository to a checkpoint compatible with your just created ü§ó Transformers implementation of\\n*brand_new_bert*. It is not advised to write the conversion script from scratch, but rather to look through already\\nexisting conversion scripts in ü§ó Transformers for one that has been used to convert a similar model that was written in\\nthe same framework as *brand_new_bert*. Usually, it is enough to copy an already existing conversion script and\\nslightly adapt it for your use case. Don\\'t hesitate to ask the Hugging Face team to point you to a similar already\\nexisting conversion script for your model.\\n\\n- If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT\\'s conversion script [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)\\n- If you are porting a model from PyTorch to PyTorch, a good starting point might be BART\\'s conversion script [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)\\n\\nIn the following, we\\'ll quickly explain how PyTorch models store layer weights and define layer names. In PyTorch, the\\nname of a layer is defined by the name of the class attribute you give the layer. Let\\'s define a dummy model in\\nPyTorch, called `SimpleModel` as follows:\\n\\n```python\\nfrom torch import nn\\n\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.dense = nn.Linear(10, 10)\\n        self.intermediate = nn.Linear(10, 10)\\n        self.layer_norm = nn.LayerNorm(10)\\n```\\n\\nNow we can create an instance of this model definition which will fill all weights: `dense`, `intermediate`,\\n`layer_norm` with random weights. We can print the model to see its architecture\\n\\n```python\\nmodel = SimpleModel()\\n\\nprint(model)\\n```\\n\\nThis will print out the following:\\n\\n```\\nSimpleModel(\\n  (dense): Linear(in_features=10, out_features=10, bias=True)\\n  (intermediate): Linear(in_features=10, out_features=10, bias=True)\\n  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\\n)\\n```\\n\\nWe can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight\\nvalues of a specific layer:\\n\\n```python\\nprint(model.dense.weight.data)\\n```\\n\\nto see that the weights were randomly initialized\\n\\n```\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0.2077,  0.2157],\\n        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,\\n          0.2166, -0.0212],\\n        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,\\n         -0.1023, -0.0447],\\n        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,\\n         -0.1876, -0.2467],\\n        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\\n          0.2577,  0.0402],\\n        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,\\n          0.2132,  0.1680],\\n        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,\\n          0.2707, -0.2509],\\n        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\\n          0.1829, -0.1568],\\n        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,\\n          0.0333, -0.0536],\\n        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,\\n          0.2220,  0.2358]]).\\n```\\n\\nIn the conversion script, you should fill those randomly initialized weights with the exact weights of the\\ncorresponding layer in the checkpoint. *E.g.*\\n\\n```python\\n# retrieve matching layer weights, e.g. by\\n# recursive algorithm\\nlayer_name = \"dense\"\\npretrained_weight = array_of_dense_layer\\n\\nmodel_pointer = getattr(model, \"dense\")\\n\\nmodel_pointer.weight.data = torch.from_numpy(pretrained_weight)\\n```\\n\\nWhile doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding\\npretrained checkpoint weight exactly match in both **shape and name**. To do so, it is **necessary** to add assert\\nstatements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:\\n\\n```python\\nassert (\\n    model_pointer.weight.shape == pretrained_weight.shape\\n), f\"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched\"\\n```\\n\\nBesides, you should also print out the names of both weights to make sure they match, *e.g.*\\n\\n```python\\nlogger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")\\n```\\n\\nIf either the shape or the name doesn\\'t match, you probably assigned the wrong checkpoint weight to a randomly\\ninitialized layer of the ü§ó Transformers implementation.\\n\\nAn incorrect shape is most likely due to an incorrect setting of the config parameters in `BrandNewBertConfig()` that\\ndo not exactly match those that were used for the checkpoint you want to convert. However, it could also be that\\nPyTorch\\'s implementation of a layer requires the weight to be transposed beforehand.\\n\\nFinally, you should also check that **all** required weights are initialized and print out all checkpoint weights that\\nwere not used for initialization to make sure the model is correctly converted. It is completely normal, that the\\nconversion trials fail with either a wrong shape statement or wrong name assignment. This is most likely because either\\nyou used incorrect parameters in `BrandNewBertConfig()`, have a wrong architecture in the ü§ó Transformers\\nimplementation, you have a bug in the `init()` functions of one of the components of the ü§ó Transformers\\nimplementation or you need to transpose one of the checkpoint weights.\\n\\nThis step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the\\nTransformers model. Having correctly loaded the checkpoint into the ü§ó Transformers implementation, you can then save\\nthe model under a folder of your choice `/path/to/converted/checkpoint/folder` that should then contain both a\\n`pytorch_model.bin` file and a `config.json` file:\\n\\n```python\\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\\n```\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into the ü§ó Transformers implementation, you should now make\\nsure that the forward pass is correctly implemented. In [Get familiar with the original repository](#34-run-a-pretrained-checkpoint-using-the-original-repository), you have already created a script that runs a forward\\npass of the model using the original repository. Now you should write an analogous script using the ü§ó Transformers\\nimplementation instead of the original one. It should look as follows:\\n\\n```python\\nmodel = BrandNewBertModel.from_pretrained(\"/path/to/converted/checkpoint/folder\")\\ninput_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\\noutput = model(input_ids).last_hidden_states\\n```\\n\\nIt is very likely that the ü§ó Transformers implementation and the original model implementation don\\'t give the exact\\nsame output the very first time or that the forward pass throws an error. Don\\'t be disappointed - it\\'s expected! First,\\nyou should make sure that the forward pass doesn\\'t throw any errors. It often happens that the wrong dimensions are\\nused leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`\\ninstead of `torch.float32`. Don\\'t hesitate to ask the Hugging Face team for help, if you don\\'t manage to solve\\ncertain errors.\\n\\nThe final part to make sure the ü§ó Transformers implementation works correctly is to ensure that the outputs are\\nequivalent to a precision of `1e-3`. First, you should ensure that the output shapes are identical, *i.e.*\\n`outputs.shape` should yield the same value for the script of the ü§ó Transformers implementation and the original\\nimplementation. Next, you should make sure that the output values are identical as well. This one of the most difficult\\nparts of adding a new model. Common mistakes why the outputs are not identical are:\\n\\n- Some layers were not added, *i.e.* an *activation* layer was not added, or the residual connection was forgotten\\n- The word embedding matrix was not tied\\n- The wrong positional embeddings are used because the original implementation uses on offset\\n- Dropout is applied during the forward pass. To fix this make sure *model.training is False* and that no dropout\\n  layer is falsely activated during the forward pass, *i.e.* pass *self.training* to [PyTorch\\'s functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)\\n\\nThe best way to fix the problem is usually to look at the forward pass of the original implementation and the ü§ó\\nTransformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out\\nintermediate outputs of both implementations of the forward pass to find the exact position in the network where the ü§ó\\nTransformers implementation shows a different output than the original implementation. First, make sure that the\\nhard-coded `input_ids` in both scripts are identical. Next, verify that the outputs of the first transformation of\\nthe `input_ids` (usually the word embeddings) are identical. And then work your way up to the very last layer of the\\nnetwork. At some point, you will notice a difference between the two implementations, which should point you to the bug\\nin the ü§ó Transformers implementation. From our experience, a simple and efficient way is to add many print statements\\nin both the original implementation and ü§ó Transformers implementation, at the same positions in the network\\nrespectively, and to successively remove print statements showing the same values for intermediate presentations.\\n\\nWhen you\\'re confident that both implementations yield the same output, verifying the outputs with\\n`torch.allclose(original_output, output, atol=1e-3)`, you\\'re done with the most difficult part! Congratulations - the\\nwork left to be done should be a cakewalk üòä.\\n\\n**8. Adding all necessary model tests**\\n\\nAt this point, you have successfully added a new model. However, it is very much possible that the model does not yet\\nfully comply with the required design. To make sure, the implementation is fully compatible with ü§ó Transformers, all\\ncommon tests should pass. The Cookiecutter should have automatically added a test file for your model, probably under\\nthe same `tests/models/brand_new_bert/test_modeling_brand_new_bert.py`. Run this test file to verify that all common\\ntests pass:\\n\\n```bash\\npytest tests/models/brand_new_bert/test_modeling_brand_new_bert.py\\n```\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the nice work you have done is well tested, so that\\n\\n- a) The community can easily understand your work by looking at specific tests of *brand_new_bert*\\n- b) Future changes to your model will not break any important feature of the model.\\n\\nAt first, integration tests should be added. Those integration tests essentially do the same as the debugging scripts\\nyou used earlier to implement the model to ü§ó Transformers. A template of those model tests is already added by the\\nCookiecutter, called `BrandNewBertModelIntegrationTests` and only has to be filled out by you. To ensure that those\\ntests are passing, run\\n\\n```bash\\nRUN_SLOW=1 pytest -sv tests/models/brand_new_bert/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests\\n```\\n\\n<Tip>\\n\\nIn case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n\\n</Tip>\\n\\nSecond, all features that are special to *brand_new_bert* should be tested additionally in a separate test under\\n`BrandNewBertModelTester`/``BrandNewBertModelTest`. This part is often forgotten but is extremely useful in two\\nways:\\n\\n- It helps to transfer the knowledge you have acquired during the model addition to the community by showing how the\\n  special features of *brand_new_bert* should work.\\n- Future contributors can quickly test changes to the model by running those special tests.\\n\\n\\n**9. Implement the tokenizer**\\n\\nNext, we should add the tokenizer of *brand_new_bert*. Usually, the tokenizer is equivalent or very similar to an\\nalready existing tokenizer of ü§ó Transformers.\\n\\nIt is very important to find/extract the original tokenizer file and to manage to load this file into the ü§ó\\nTransformers\\' implementation of the tokenizer.\\n\\nTo ensure that the tokenizer works correctly, it is recommended to first create a script in the original repository\\nthat inputs a string and returns the `input_ids``. It could look similar to this (in pseudo-code):\\n\\n```python\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\nmodel = BrandNewBertModel.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = model.tokenize(input_str)\\n```\\n\\nYou might have to take a deeper look again into the original repository to find the correct tokenizer function or you\\nmight even have to do changes to your clone of the original repository to only output the `input_ids`. Having written\\na functional tokenization script that uses the original repository, an analogous script for ü§ó Transformers should be\\ncreated. It should look similar to this:\\n\\n```python\\nfrom transformers import BrandNewBertTokenizer\\n\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\n\\ntokenizer = BrandNewBertTokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\\n\\ninput_ids = tokenizer(input_str).input_ids\\n```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer test file should also be added.\\n\\nAnalogous to the modeling test files of *brand_new_bert*, the tokenization test files of *brand_new_bert* should\\ncontain a couple of hard-coded integration tests.\\n\\n**10. Run End-to-end integration tests**\\n\\nHaving added the tokenizer, you should also add a couple of end-to-end integration tests using both the model and the\\ntokenizer to `tests/models/brand_new_bert/test_modeling_brand_new_bert.py` in ü§ó Transformers.\\nSuch a test should show on a meaningful\\ntext-to-text sample that the ü§ó Transformers implementation works as expected. A meaningful text-to-text sample can\\ninclude *e.g.* a source-to-target-translation pair, an article-to-summary pair, a question-to-answer pair, etc‚Ä¶ If none\\nof the ported checkpoints has been fine-tuned on a downstream task it is enough to simply rely on the model tests. In a\\nfinal step to ensure that the model is fully functional, it is advised that you also run all tests on GPU. It can\\nhappen that you forgot to add some `.to(self.device)` statements to internal tensors of the model, which in such a\\ntest would show in an error. In case you have no access to a GPU, the Hugging Face team can take care of running those\\ntests for you.\\n\\n**11. Add Docstring**\\n\\nNow, all the necessary functionality for *brand_new_bert* is added - you\\'re almost done! The only thing left to add is\\na nice docstring and a doc page. The Cookiecutter should have added a template file called\\n`docs/source/model_doc/brand_new_bert.mdx` that you should fill out. Users of your model will usually first look at\\nthis page before using your model. Hence, the documentation must be understandable and concise. It is very useful for\\nthe community to add some *Tips* to show how the model should be used. Don\\'t hesitate to ping the Hugging Face team\\nregarding the docstrings.\\n\\nNext, make sure that the docstring added to `src/transformers/models/brand_new_bert/modeling_brand_new_bert.py` is\\ncorrect and included all necessary inputs and outputs. We have a detailed guide about writing documentation and our docstring format [here](writing-documentation). It is always to good to remind oneself that documentation should\\nbe treated at least as carefully as the code in ü§ó Transformers since the documentation is usually the first contact\\npoint of the community with the model.\\n\\n**Code refactor**\\n\\nGreat, now you have added all the necessary code for *brand_new_bert*. At this point, you should correct some potential\\nincorrect code style by running:\\n\\n```bash\\nmake style\\n```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality\\n```\\n\\nThere are a couple of other very strict design tests in ü§ó Transformers that might still be failing, which shows up in\\nthe tests of your pull request. This is often because of some missing information in the docstring or some incorrect\\nnaming. The Hugging Face team will surely help you if you\\'re stuck here.\\n\\nLastly, it is always a good idea to refactor one\\'s code after having ensured that the code works correctly. With all\\ntests passing, now it\\'s a good time to go over the added code again and do some refactoring.\\n\\nYou have now finished the coding part, congratulation! üéâ You are Awesome! üòé\\n\\n**12. Upload the models to the model hub**\\n\\nIn this final part, you should convert and upload all checkpoints to the model hub and add a model card for each\\nuploaded model checkpoint. You can get familiar with the hub functionalities by reading our [Model sharing and uploading Page](model_sharing). You should work alongside the Hugging Face team here to decide on a fitting name for each\\ncheckpoint and to get the required access rights to be able to upload the model under the author\\'s organization of\\n*brand_new_bert*. The `push_to_hub` method, present in all models in `transformers`, is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:\\n\\n```python\\nbrand_new_bert.push_to_hub(\"brand_new_bert\")\\n# Uncomment the following line to push to an organization.\\n# brand_new_bert.push_to_hub(\"<organization>/brand_new_bert\")\\n```\\n\\nIt is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the\\nspecific characteristics of this particular checkpoint, *e.g.* On which dataset was the checkpoint\\npretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to\\ncorrectly use the model.\\n\\n**13. (Optional) Add notebook**\\n\\nIt is very helpful to add a notebook that showcases in-detail how *brand_new_bert* can be used for inference and/or\\nfine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.\\n\\n**14. Submit your finished PR**\\n\\nYou\\'re done programming now and can move to the last step, which is getting your PR merged into main. Usually, the\\nHugging Face team should have helped you already at this point, but it is worth taking some time to give your finished\\nPR a nice description and eventually add comments to your code, if you want to point out certain design choices to your\\nreviewer.\\n\\n### Share your work!!\\n\\nNow, it\\'s time to get some credit from the community for your work! Having completed a model addition is a major\\ncontribution to Transformers and the whole NLP community. Your code and the ported pre-trained models will certainly be\\nused by hundreds and possibly even thousands of developers and researchers. You should be proud of your work and share\\nyour achievement with the community.\\n\\n**You have made another model that is super easy to access for everyone in the community! ü§Ø**\\n' metadata={'title': 'add_new_model.mdx', 'repo_owner': 'huggingface', 'repo_name': 'transformers'}\n",
      "page_content='<!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n-->\\n\\n# How to create a custom pipeline?\\n\\nIn this guide, we will see how to create a custom pipeline and share it on the [Hub](hf.co/models) or add it to the\\nü§ó Transformers library.\\n\\nFirst and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes,\\ndictionaries or whatever seems to be the most likely desired input. Try to keep these inputs as pure Python as possible\\nas it makes compatibility easier (even through other languages via JSON). Those will be the `inputs` of the\\npipeline (`preprocess`).\\n\\nThen define the `outputs`. Same policy as the `inputs`. The simpler, the better. Those will be the outputs of\\n`postprocess` method.\\n\\nStart by inheriting the base class `Pipeline` with the 4 methods needed to implement `preprocess`,\\n`_forward`, `postprocess`, and `_sanitize_parameters`.\\n\\n\\n```python\\nfrom transformers import Pipeline\\n\\n\\nclass MyPipeline(Pipeline):\\n    def _sanitize_parameters(self, **kwargs):\\n        preprocess_kwargs = {}\\n        if \"maybe_arg\" in kwargs:\\n            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\\n        return preprocess_kwargs, {}, {}\\n\\n    def preprocess(self, inputs, maybe_arg=2):\\n        model_input = Tensor(inputs[\"input_ids\"])\\n        return {\"model_input\": model_input}\\n\\n    def _forward(self, model_inputs):\\n        # model_inputs == {\"model_input\": model_input}\\n        outputs = self.model(**model_inputs)\\n        # Maybe {\"logits\": Tensor(...)}\\n        return outputs\\n\\n    def postprocess(self, model_outputs):\\n        best_class = model_outputs[\"logits\"].softmax(-1)\\n        return best_class\\n```\\n\\nThe structure of this breakdown is to support relatively seamless support for CPU/GPU, while supporting doing\\npre/postprocessing on the CPU on different threads\\n\\n`preprocess` will take the originally defined inputs, and turn them into something feedable to the model. It might\\ncontain more information and is usually a `Dict`.\\n\\n`_forward` is the implementation detail and is not meant to be called directly. `forward` is the preferred\\ncalled method as it contains safeguards to make sure everything is working on the expected device. If anything is\\nlinked to a real model it belongs in the `_forward` method, anything else is in the preprocess/postprocess.\\n\\n`postprocess` methods will take the output of `_forward` and turn it into the final output that was decided\\nearlier.\\n\\n`_sanitize_parameters` exists to allow users to pass any parameters whenever they wish, be it at initialization\\ntime `pipeline(...., maybe_arg=4)` or at call time `pipe = pipeline(...); output = pipe(...., maybe_arg=4)`.\\n\\nThe returns of `_sanitize_parameters` are the 3 dicts of kwargs that will be passed directly to `preprocess`,\\n`_forward`, and `postprocess`. Don\\'t fill anything if the caller didn\\'t call with any extra parameter. That\\nallows to keep the default arguments in the function definition which is always more \"natural\".\\n\\nA classic example would be a `top_k` argument in the post processing in classification tasks.\\n\\n```python\\n>>> pipe = pipeline(\"my-new-task\")\\n>>> pipe(\"This is a test\")\\n[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}, {\"label\": \"3-star\", \"score\": 0.05}\\n{\"label\": \"4-star\", \"score\": 0.025}, {\"label\": \"5-star\", \"score\": 0.025}]\\n\\n>>> pipe(\"This is a test\", top_k=2)\\n[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}]\\n```\\n\\nIn order to achieve that, we\\'ll update our `postprocess` method with a default parameter to `5`. and edit\\n`_sanitize_parameters` to allow this new parameter.\\n\\n\\n```python\\ndef postprocess(self, model_outputs, top_k=5):\\n    best_class = model_outputs[\"logits\"].softmax(-1)\\n    # Add logic to handle top_k\\n    return best_class\\n\\n\\ndef _sanitize_parameters(self, **kwargs):\\n    preprocess_kwargs = {}\\n    if \"maybe_arg\" in kwargs:\\n        preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\\n\\n    postprocess_kwargs = {}\\n    if \"top_k\" in kwargs:\\n        postprocess_kwargs[\"top_k\"] = kwargs[\"top_k\"]\\n    return preprocess_kwargs, {}, postprocess_kwargs\\n```\\n\\nTry to keep the inputs/outputs very simple and ideally JSON-serializable as it makes the pipeline usage very easy\\nwithout requiring users to understand new kind of objects. It\\'s also relatively common to support many different types\\nof arguments for ease of use (audio files, can be filenames, URLs or pure bytes)\\n\\n\\n\\n## Adding it to the list of supported tasks\\n\\nTo register your `new-task` to the list of supported tasks, you have to add it to the `PIPELINE_REGISTRY`:\\n\\n```python\\nfrom transformers.pipelines import PIPELINE_REGISTRY\\n\\nPIPELINE_REGISTRY.register_pipeline(\\n    \"new-task\",\\n    pipeline_class=MyPipeline,\\n    pt_model=AutoModelForSequenceClassification,\\n)\\n```\\n\\nYou can specify a default model if you want, in which case it should come with a specific revision (which can be the name of a branch or a commit hash, here we took `\"abcdef\"`) as well as the type:\\n\\n```python\\nPIPELINE_REGISTRY.register_pipeline(\\n    \"new-task\",\\n    pipeline_class=MyPipeline,\\n    pt_model=AutoModelForSequenceClassification,\\n    default={\"pt\": (\"user/awesome_model\", \"abcdef\")},\\n    type=\"text\",  # current support type: text, audio, image, multimodal\\n)\\n```\\n\\n## Share your pipeline on the Hub\\n\\nTo share your custom pipeline on the Hub, you just have to save the custom code of your `Pipeline` subclass in a\\npython file. For instance, let\\'s say we want to use a custom pipeline for sentence pair classification like this:\\n\\n```py\\nimport numpy as np\\n\\nfrom transformers import Pipeline\\n\\n\\ndef softmax(outputs):\\n    maxes = np.max(outputs, axis=-1, keepdims=True)\\n    shifted_exp = np.exp(outputs - maxes)\\n    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\\n\\n\\nclass PairClassificationPipeline(Pipeline):\\n    def _sanitize_parameters(self, **kwargs):\\n        preprocess_kwargs = {}\\n        if \"second_text\" in kwargs:\\n            preprocess_kwargs[\"second_text\"] = kwargs[\"second_text\"]\\n        return preprocess_kwargs, {}, {}\\n\\n    def preprocess(self, text, second_text=None):\\n        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)\\n\\n    def _forward(self, model_inputs):\\n        return self.model(**model_inputs)\\n\\n    def postprocess(self, model_outputs):\\n        logits = model_outputs.logits[0].numpy()\\n        probabilities = softmax(logits)\\n\\n        best_class = np.argmax(probabilities)\\n        label = self.model.config.id2label[best_class]\\n        score = probabilities[best_class].item()\\n        logits = logits.tolist()\\n        return {\"label\": label, \"score\": score, \"logits\": logits}\\n```\\n\\nThe implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If we have saved this in\\na file named `pair_classification.py`, we can then import it and register it like this:\\n\\n```py\\nfrom pair_classification import PairClassificationPipeline\\nfrom transformers.pipelines import PIPELINE_REGISTRY\\nfrom transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\\n\\nPIPELINE_REGISTRY.register_pipeline(\\n    \"pair-classification\",\\n    pipeline_class=PairClassificationPipeline,\\n    pt_model=AutoModelForSequenceClassification,\\n    tf_model=TFAutoModelForSequenceClassification,\\n)\\n```\\n\\nOnce this is done, we can use it with a pretrained model. For instance `sgugger/finetuned-bert-mrpc` has been\\nfine-tuned on the MRPC dataset, which classifies pairs of sentences as paraphrases or not.\\n\\n```py\\nfrom transformers import pipeline\\n\\nclassifier = pipeline(\"pair-classification\", model=\"sgugger/finetuned-bert-mrpc\")\\n```\\n\\nThen we can share it on the Hub by using the `save_pretrained` method in a `Repository`:\\n\\n```py\\nfrom huggingface_hub import Repository\\n\\nrepo = Repository(\"test-dynamic-pipeline\", clone_from=\"{your_username}/test-dynamic-pipeline\")\\nclassifier.save_pretrained(\"test-dynamic-pipeline\")\\nrepo.push_to_hub()\\n```\\n\\nThis will copy the file where you defined `PairClassificationPipeline` inside the folder `\"test-dynamic-pipeline\"`,\\nalong with saving the model and tokenizer of the pipeline, before pushing everything in the repository\\n`{your_username}/test-dynamic-pipeline`. After that anyone can use it as long as they provide the option\\n`trust_remote_code=True`:\\n\\n```py\\nfrom transformers import pipeline\\n\\nclassifier = pipeline(model=\"{your_username}/test-dynamic-pipeline\", trust_remote_code=True)\\n```\\n\\n## Add the pipeline to ü§ó Transformers\\n\\nIf you want to contribute your pipeline to ü§ó Transformers, you will need to add a new module in the `pipelines` submodule\\nwith the code of your pipeline, then add it in the list of tasks defined in `pipelines/__init__.py`.\\n\\nThen you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py` with example with the other tests.\\n\\nThe `run_pipeline_test` function will be very generic and run on small random models on every possible\\narchitecture as defined by `model_mapping` and `tf_model_mapping`.\\n\\nThis is very important to test future compatibility, meaning if someone adds a new model for\\n`XXXForQuestionAnswering` then the pipeline test will attempt to run on it. Because the models are random it\\'s\\nimpossible to check for actual values, that\\'s why there is a helper `ANY` that will simply attempt to match the\\noutput of the pipeline TYPE.\\n\\nYou also *need* to implement 2 (ideally 4) tests.\\n\\n- `test_small_model_pt` : Define 1 small model for this pipeline (doesn\\'t matter if the results don\\'t make sense)\\n  and test the pipeline outputs. The results should be the same as `test_small_model_tf`.\\n- `test_small_model_tf` : Define 1 small model for this pipeline (doesn\\'t matter if the results don\\'t make sense)\\n  and test the pipeline outputs. The results should be the same as `test_small_model_pt`.\\n- `test_large_model_pt` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\\n  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\\n  sure there is no drift in future releases.\\n- `test_large_model_tf` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to\\n  make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make\\n  sure there is no drift in future releases.\\n' metadata={'title': 'add_new_pipeline.mdx', 'repo_owner': 'huggingface', 'repo_name': 'transformers'}\n"
     ]
    }
   ],
   "source": [
    "for doc in data:\n",
    "  print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eih-71yDvdqW"
   },
   "source": [
    "## Text Splitters\n",
    "\n",
    "Imagina que est√°s trabajando con un libro muy grueso y necesitas pasarlo por una ventana muy estrecha. ¬øQu√© har√≠as? Probablemente, lo cortar√≠as en secciones m√°s manejables y las pasar√≠as una por una. Ahora, cambia el libro por un documento largo y la ventana por el modelo de procesamiento de lenguaje natural que est√°s utilizando. Este escenario es exactamente por qu√© necesitamos los separadores de texto en el campo de la inteligencia artificial.\n",
    "\n",
    "LangChain, comprendiendo este desaf√≠o, tiene incorporados varios separadores de texto para facilitar la divisi√≥n, combinaci√≥n, filtrado y manipulaci√≥n de los documentos. De este modo, puedes transformarlos para que se adapten mejor a tu aplicaci√≥n.\n",
    "\n",
    "Cuando nos enfrentamos a textos largos, es imprescindible dividirlos en fragmentos. Aunque esto suena sencillo, no es tan simple como parece. Queremos mantener las partes del texto que est√°n sem√°nticamente relacionadas juntas. Y esto de \"sem√°nticamente relacionado\" puede variar dependiendo del tipo de texto con el que est√©s trabajando.\n",
    "\n",
    "Piensa en el texto como un rompecabezas, cada pieza (o fragmento) tiene su propio significado, pero tambi√©n contribuye a la imagen general (o el contexto). Queremos separar el rompecabezas en piezas, pero sin perder el sentido de la imagen completa.\n",
    "\n",
    "Entonces, ¬øc√≥mo funcionan exactamente los separadores de texto?\n",
    "\n",
    "1. Primero, dividen el texto en fragmentos peque√±os y sem√°nticamente significativos (a menudo oraciones).\n",
    "2. Luego, comienzan a combinar estos fragmentos peque√±os en un fragmento m√°s grande hasta que alcanzan un tama√±o determinado (medido por alguna funci√≥n).\n",
    "3. Una vez que alcanzan ese tama√±o, hacen de ese fragmento su propio texto y luego comienzan a crear un nuevo fragmento de texto con cierta superposici√≥n. Esto es para mantener el contexto entre fragmentos.\n",
    "\n",
    "En este proceso, puedes personalizar tu separador de texto en dos aspectos: c√≥mo se divide el texto y c√≥mo se mide el tama√±o del fragmento.\n",
    "\n",
    "## RecursiveCharacterTextSplitter\n",
    "\n",
    "Para facilitar las cosas, LangChain ofrece un separador de texto por defecto: el `RecursiveCharacterTextSplitter`. Este separador de texto toma una lista de caracteres y trata de crear fragmentos bas√°ndose en la divisi√≥n del primer car√°cter. Pero, si alg√∫n fragmento resulta demasiado grande, pasa al siguiente car√°cter, y as√≠ sucesivamente. Los caracteres que intenta dividir son [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    "El `RecursiveCharacterTextSplitter` ofrece una ventaja importante: intenta preservar tanto contexto sem√°ntico como sea posible manteniendo intactos los p√°rrafos, las oraciones y las palabras. Estas unidades de texto suelen tener fuertes relaciones sem√°nticas, lo que significa que las palabras dentro de ellas a menudo est√°n estrechamente relacionadas en significado. Esta es una caracter√≠stica sumamente beneficiosa para muchas tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "Piensa en una conversaci√≥n cotidiana, es m√°s f√°cil entender una idea cuando escuchas la oraci√≥n completa en lugar de palabras o frases sueltas. Esta misma l√≥gica se aplica a los modelos de procesamiento de lenguaje natural. Al mantener intactos los p√°rrafos, oraciones y palabras, se preserva el 'flujo de conversaci√≥n' en el texto, lo que puede mejorar la eficacia del modelo al interpretar y comprender el texto.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4sa5h0T6Q2p"
   },
   "source": [
    "A partir de nuestros `Document` podemos crear m√°s `Document` con `RecursiveCharacterTextSplitter`, es decir, podemos partirlos manteniendo nuestra metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-y2LonVykDz"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    length_function=len,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPRu6aA_3Y9d",
    "outputId": "905f5536-af61-4e92-c082-9b89fe590e5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IB_IWdU1Tu2",
    "outputId": "ed54e01a-0fac-4281-e7a8-4fb788a959ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='stances. Suppose, for example, that the plaintext of each \\npuzzle is 96 bits, consisting of 64 bits of key together with \\nathirty-two bit block of zeros that enables Bob to recognize \\nthe right solution. The puzzle is constructed by encrypting \\nthis plaintext using a block cipher with 20 bits of key. Alice produces a million of these puzzles and Bob requires about \\nhalf a million tests to solve one. The bandwidth and com- \\nputing power required to make this feasible are large but \\nnot inaccessible. On a DSI (1.544 Mbit) channel it would \\nrequire about a minute to communicate the puzzles. If keys \\ncan be tried on the selected puzzle at about ten-thousand \\nper second, it will take Bob another minute to solve it. \\nFinally, it will take a similar amount of time for Alice to figure \\nout, from the test message, which key has been chosen. \\nThe intruder can expect to have to solve half a million \\npuzzles at half a million tries apiece. With equivalent com-', metadata={'source': './public_key_cryptography.pdf', 'page': 2})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y62IIHUw7QXU"
   },
   "source": [
    "### Tama√±o del fragmento y superposici√≥n\n",
    "\n",
    "Imagina que est√°s trabajando con un rompecabezas de palabras, donde cada pieza es una porci√≥n de texto. Para que este rompecabezas sea manejable, necesitas asegurarte de que las piezas son del tama√±o correcto y se superponen adecuadamente. En el mundo del procesamiento de texto, estas \"piezas\" son los fragmentos de texto, y su tama√±o y superposici√≥n pueden ser esenciales para el rendimiento de tus modelos de aprendizaje autom√°tico.\n",
    "\n",
    "En primer lugar, hablemos del tama√±o del fragmento. La pregunta que podr√≠as hacerte es, ¬øcu√°n grande debe ser cada fragmento de texto? Bien, la respuesta depende del modelo de embedding de texto que est√©s utilizando. Un \"modelo de embedding\" puede parecer un t√©rmino intimidante, pero es simplemente una herramienta que convertimos palabras, oraciones o documentos completos en vectores num√©ricos que las m√°quinas pueden entender.\n",
    "\n",
    "Por ejemplo, el modelo de incrustaci√≥n `text-embedding-ada-002` de OpenAI es excelente para muchas aplicaciones, pero puede manejar hasta 8191 tokens. Ahora, podr√≠as preguntarte, ¬øqu√© es un 'token'? Un token no es lo mismo que un car√°cter. Un token puede ser una palabra o incluso un signo de puntuaci√≥n. Por lo tanto, un token podr√≠a tener desde un solo car√°cter hasta una decena de ellos. De esta manera, tu fragmento de texto podr√≠a tener miles de caracteres, pero debes asegurarte de que no contenga m√°s de 8191 tokens.\n",
    "\n",
    "Mantener los fragmentos entre 500 y 1000 caracteres suele ser un buen equilibrio. Este tama√±o asegura que el contenido sem√°ntico es preservado sin sobrepasar el l√≠mite de tokens del modelo.\n",
    "\n",
    "En cuanto a la superposici√≥n, este par√°metro decide cu√°nto texto queremos repetir entre fragmentos. ¬øPor qu√© querr√≠amos hacer esto? Bueno, la superposici√≥n ayuda a mantener el contexto entre fragmentos contiguos. Es como tener una peque√±a ventana de memoria que se traslada de un fragmento a otro. Generalmente, se recomienda ajustar la superposici√≥n al 10-20% del tama√±o del fragmento. Esto asegura cierta conexi√≥n entre los fragmentos sin causar demasiada repetici√≥n. Si la superposici√≥n es demasiado grande, puede ralentizar el proceso y aumentar los costos de procesamiento.\n",
    "\n",
    "Por lo tanto, si est√°s lidiando con textos relativamente largos, esta es la configuraci√≥n que podr√≠as utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akveOXWG7jF6"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 50,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# o\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size = 1000,\n",
    "#     chunk_overlap  = 100,\n",
    "#     length_function = len,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23-uHWnI7sli"
   },
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "JNeqJSgy7whA",
    "outputId": "fb0037c5-b586-4d86-82b2-3f4367c40a44"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The First Ten Years of Public-Key \\nCryptography \\nWH lTFl ELD DI FFlE \\nInvited Paper \\nPublic-key cryptosystems separate the capacities for encryption \\nand decryption so that 7) many people can encrypt messages in \\nsuch a way that only one person can read them, or 2) one person \\ncan encrypt messages in such a way that many people can read \\nthem. This separation allows important improvements in the man- \\nagement of cryptographic keys and makes it possible to ‚Äòsign‚Äô a \\npurely digital message.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAK2hv5rGgDV"
   },
   "source": [
    "## Modelos de embeddings\n",
    "Los modelos de incrustaciones de texto son fundamentales en el procesamiento de lenguaje natural (NLP). Transforman palabras, frases y documentos en representaciones vectoriales que capturan su significado y las relaciones sem√°nticas entre ellas. Esto posibilita que los algoritmos de aprendizaje autom√°tico procesen texto y realicen operaciones como la b√∫squeda sem√°ntica, que se basa en la similitud de los textos en el espacio vectorial.\n",
    "\n",
    "### Comprendiendo el espacio de alta dimensi√≥n\n",
    "\n",
    "En un espacio de alta dimensi√≥n, cada dimensi√≥n representa una caracter√≠stica √∫nica de los datos. Al igual que utilizamos longitud, anchura y altura para localizar una posici√≥n en un espacio tridimensional, en un espacio de alta dimensi√≥n usamos m√∫ltiples dimensiones para ubicar y describir un punto de datos.\n",
    "\n",
    "Las incrustaciones de vectores, por tanto, son como 'direcciones' num√©ricas para puntos de datos en este espacio. As√≠, un espacio vectorial en el que mapeamos palabras relacionadas con emociones, podr√≠a tener dimensiones para capturar cu√°n 'feliz' es una palabra, la intensidad de la emoci√≥n, si es una emoci√≥n positiva o negativa, etc. Cuantas m√°s dimensiones usemos, m√°s caracter√≠sticas podremos encapsular de cada palabra.\n",
    "\n",
    "### Los embeddings: herramientas esenciales para la comprensi√≥n del lenguaje\n",
    "\n",
    "La clase `Embeddings` de LangChain proporciona una interfaz para trabajar con modelos de incrustaciones de texto. Esta clase no est√° vinculada a un proveedor espec√≠fico de modelos de incrustaciones, sino que ofrece una interfaz est√°ndar para interactuar con varios proveedores como OpenAI, Cohere y Hugging Face.\n",
    "\n",
    "Las incrustaciones de texto son como un traductor que transforma las palabras, frases y documentos en representaciones num√©ricas de tama√±o fijo que capturan su significado y estructura.\n",
    "\n",
    "Por ejemplo, una oraci√≥n como \"Esto es c√≥mo funcionan las incrustaciones\" se procesa de la siguiente manera:\n",
    "\n",
    "1. Se tokeniza la oraci√≥n en palabras individuales: [\"Esto\", \"es\", \"c√≥mo\", \"funcionan\", \"las\", \"incrustaciones\"].\n",
    "2. Un modelo de incrustaciones pre-entrenado convierte cada palabra en su vector de incrustaciones correspondiente, representado como una matriz de n√∫meros de longitud fija.\n",
    "\n",
    "De esta manera, la oraci√≥n se convierte en una secuencia de vectores num√©ricos, y sobre estos vectores podemos realizar operaciones poderosas como b√∫squedas sem√°nticas, recuperando los resultados m√°s relevantes basados en la similitud entre las incrustaciones.\n",
    "\n",
    "### **La clase embeddings en LangChain**\n",
    "\n",
    "En LangChain la clase base **`Embeddings`** proporciona dos m√©todos:\n",
    "\n",
    "- uno para incrustar documentos.\n",
    "- otro para incrustar consultas.\n",
    "\n",
    "El primer m√©todo acepta m√∫ltiples textos, mientras que el segundo solo uno. Esto se debe a que algunos proveedores de incrustaciones tienen diferentes m√©todos para los documentos y las consultas.\n",
    "\n",
    "### **Integraci√≥n con proveedores de modelos de incrustaciones**\n",
    "\n",
    "LangChain integra una variedad de proveedores de modelos de incrustaciones de texto, incluyendo:\n",
    "\n",
    "- Aleph Alpha\n",
    "- AzureOpenAI\n",
    "- Cohere\n",
    "- Fake Embeddings\n",
    "- Hugging Face Hub\n",
    "- InstructEmbeddings\n",
    "- Jina\n",
    "- Llama-cpp\n",
    "- OpenAI\n",
    "- SageMaker Endpoint Embeddings\n",
    "- Self Hosted Embeddings\n",
    "- Sentence Transformers Embeddings\n",
    "- TensorFlow Hub\n",
    "\n",
    "Estos proveedores ofrecen una gran variedad de opciones, permiti√©ndote elegir el modelo de incrustaciones que mejor se adapte a tus necesidades. En futuras secciones, profundizaremos en c√≥mo usar estos proveedores de modelos de incrustaciones para mejorar el procesamiento de texto en LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypsfrhSIJCr8"
   },
   "source": [
    "### OpenAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRUOCVwuJLfD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXZ6UnS_JFCb",
    "outputId": "f5c5b9bc-42ee-4b7e-d9b5-1e7c32029325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the secret value: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = getpass('Enter the secret value: ')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GFRHXW0JYtu"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Las capacidades multilingues de \"text-embedding-ada-002\" no son claras\n",
    "embedding_openai = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RNhLBOexLe3"
   },
   "outputs": [],
   "source": [
    "embedding_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLkJeMRsJ4fk"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahHyG9b-Jw1P"
   },
   "outputs": [],
   "source": [
    "documentos_a_incrustar = [\n",
    "    \"¬°Hola parce!\",\n",
    "    \"¬°Uy, hola!\",\n",
    "    \"¬øC√≥mo te llamas?\",\n",
    "    \"Mis parceros me dicen Omar\",\n",
    "    \"¬°Hola Mundo!\"\n",
    "  ]\n",
    "\n",
    "incrustaciones = embedding_openai.embed_documents(documentos_a_incrustar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YOXAUjFLgTw",
    "outputId": "f23618d9-729a-4c3e-b720-ff729e9cc68f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incrustaciones[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ThuBx-Tyv7h"
   },
   "outputs": [],
   "source": [
    "consulta_incrustada = embedding_openai.embed_query(documentos_a_incrustar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zham6PEuy4FY"
   },
   "outputs": [],
   "source": [
    "consulta_incrustada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGXxrqEIAb-"
   },
   "source": [
    "### Hugging Face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1h_q8oZF0N-r"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433,
     "referenced_widgets": [
      "d6b630f7fbb3465fbe663933fb6170c8",
      "d7aea08ef0e54cbba3aa9459a24817d4",
      "d6deea0b90594046951ecfc5e654b521",
      "3cca73fdb39544d9b3b782274c0de406",
      "3152ec0e4ad44b3aa1a2f53d5ad7af41",
      "a7e19de7c5de43bbab5cf0c439b61ba5",
      "b0ae98d66aa341f8a63b1b38863bff05",
      "1ed2101ceca54db0b3654b7cb1c364f4",
      "478f165791ff409caaec413bab573420",
      "4b9175170d254c169fccc3ed64df4ce2",
      "8eba065070594385b439df96f19f4f58",
      "a4e873ee30114b29ab8c6a4241fee4d0",
      "ab95e77b52384a188a3e1ef030f07670",
      "b2e708eba98b468aa5c6793e0a45d819",
      "711ffc8214604fa0b68a6c4e9f5d9e15",
      "fd2fc1e5af7c461d8cd63c4fb2f7176d",
      "add69ffb76024f4392c2ab384e748c55",
      "5317d676ab8049c09d4865415de4da06",
      "f1805ad20f01482a99fa3d23931f3100",
      "b30cfa2511b04cbdad87fa44995c8f30",
      "d7b448221a114629be4823f92902d10a",
      "6fb2bfb2867246d681716a1eb48ee19b",
      "9540564e57804c329a01629c18b9d28f",
      "5adb073f4d694cecbdd3fd4de0db4303",
      "c02a47c2688441598ca7093a0d861a62",
      "802b5e26adf04b0a9d084b82df18a916",
      "3b1bd570dbf74edeb95c0e1885fb1e97",
      "846980663df1421f91b00d8ed2f01ac5",
      "dea88cc563a446f396fdf5348e2a1330",
      "f76b4fa623b143348c1f80e45e4c5050",
      "df8893ffd8a744379bc946740e97435e",
      "9462753ab1da488b8c507debc92a4dde",
      "b92e49c29f614f50a881d6995da59ce6",
      "5aa1ca63fcc44087a6002ad58dd6ac18",
      "145f3a331e9e4e4781d1dda7626b5529",
      "87dd0d16aedc40d7bf7cdafd8b5d7767",
      "8e82ec9072d84ec88d0baac30b7647ee",
      "496fc08d70674b2c87dc2de5d637696c",
      "3b5312f3df9a4c90ae7dcb468b2d99b7",
      "3d030cc7da314781aed2ef7f16c4cd2f",
      "f6e55d36c0a64e71bf56518d1ff363de",
      "4e9cb4b6eaa945a39e7b266abc44f4a3",
      "5be840ee14ec4592928e6e2e819157a4",
      "8839b9f357bd4f29bdf4864cb6c9f4fb",
      "ed0b6d625c4d463fa1870cbda67e3832",
      "5a8bd70e2f324c2b85215f2684448ce0",
      "5d7dc8ad24fb47eebc74ee965f2e20c7",
      "bc95c240682240a6a04a23369f55e259",
      "fad346e64b6f4590840893a306877d1f",
      "5329310271004ab3844d79a6efc8ce13",
      "3feff5f853c14466bb966b9812c3d412",
      "9a3c7f0c84de4995b95a681335fad42f",
      "d6c6f62d7c6f476d8e8f31cd7a483293",
      "fe3fb12b6c454b8385f362f1e4f16575",
      "aefb73b606104d0db2e99aef63fbeed8",
      "6241e4ae3d3d434d9d39e281b48df215",
      "ce5a4463dfa14708b5f6ebb72d752e7a",
      "a9d84a7253be4f2ead5b976cbb7c72bc",
      "c0b3ee69f68d4ad59548c4b1e459182f",
      "631d1ec2a32042889b0b0d2d0aa9da26",
      "4dbbe9ce3a3c4ed0b28fe5945e03c504",
      "4b4fe610923a4d87b4ce90bee05f2a04",
      "0b95425f5b7a41d8bd2b3e7308a21bc5",
      "50cae60512a040b2b4855cb1dd7a04e4",
      "5c1c4c55d17b45f4884250ddca01a5ae",
      "725e922d2b0c49acb1581353d41dfd73",
      "3c2755e3f8c2432097d81518f86f3b87",
      "dd99af757f8843f58f6fe47ad5bc6200",
      "c04fa8c1ea66460882af55e243476214",
      "c1109553184b42b1afec45fbd40d7756",
      "7e6ca775fa464447b4ffa0d3e69bb2cd",
      "776867344d944cc982994ccdee898ebb",
      "71417eb964844cfbaa4d5136079da9c5",
      "083e5b3644e04eca8fc1863642fc82d1",
      "21e38d265a444c2a8a8a8b84321fd955",
      "4925890be8d146c08cdfe963e6314eb9",
      "ad0d700268b14dbaba28093bf0509f1f",
      "4fe8f30ae2f340abb50287854b4deb8d",
      "6f688dc29e514c2686f30f3567e7795b",
      "fed6c479f8a44c89b58bea024e0f92e2",
      "4175ea3d019f4899b3d5748fd68e75fd",
      "514d3645a83e4a9e8abd96ff4b25884e",
      "5df13b9663004662a26996ba08387e7d",
      "5a744a7af90d4a0b8b623fab40937435",
      "27f7d7b94704480b9b0dd03709c646f6",
      "c94ae775e1444a96a7641844e1748f8f",
      "d9fcffd7fcfe4e3a86348cfe7e003793",
      "a3b896f1fed1472a86a9ba627802dfdc",
      "35e560bee40e4184a75bff34c55b45c9",
      "b7938a1564df4e0a8a7c9c6fdbded644",
      "9096537638064105bdcf4d118d84e0cd",
      "33a5bda185dc41d0a6fcedd5f75a6ee9",
      "1c799ac0f7a348efb96d2fcae1f70737",
      "d1d45b765ccb4d2785844766cb4d0647",
      "64a66da389e94d9f8541b76cbf05c065",
      "fc5ed7482c7646628ae89bbea85471d2",
      "1ebd2503395247f995d198c3877881de",
      "c4aae271bbe244378cf7ac2f510a967c",
      "bec2b1d15b3241b5994658364f84da9f",
      "7a13f92e7c414d1b960edcf231f2bad0",
      "9be485ce83aa4aec8928e383a77a972a",
      "700eba91d3354b099cf18ee1fd599f4c",
      "f4d1ce5883d146d3aa7a043d5a322bea",
      "fbadfd2ff84b41ceaadb6322fbd43bce",
      "dc93d522d1f041588f9fd66ea8743901",
      "ace62e9ca07c49eb9870fe47e720d9b6",
      "736d489c47e8488ca62cbd0131533e84",
      "575d06a157b44e4aa45593217c77c82a",
      "808f0e802aaf4a2faf0b5a637f99d793",
      "804f914bea774655979500704e158041",
      "3a3e9cb65017494ca25b094c4e56d855",
      "5a667ed3431449798a57bf402fdfaec7",
      "fbe1065a8e104d8fb421f545dd439fe4",
      "ab87b149111148dc8a259d12923bff15",
      "19dc0103526746c8bc86201a048a1a8c",
      "93a0ef0756ae4be3a9d9276198944a86",
      "305eb8039a164bc1860e4709db2938bd",
      "7e486346ceb2407293fbd38e23fd087d",
      "ed7bdac1045c488d83356ede827a42cc",
      "ea7712d06c6a4d1fb6a6207625ed0e48",
      "0c82982628f84b38a3e4fbe7056d047d",
      "d59c747b566d46e882cc6fd0cff84fca",
      "a88eb6ef748f41608b1a61b1177b7ad0",
      "9fbbaaf05d4049b697f559de065b5d37",
      "40d1c725603c44ad978c64557a878862",
      "4904e090e7bb486ab649844a0acde20f",
      "5b315f7242fa4226a51716baaa017c1f",
      "c13d39b8a3e64df39d798effadc31e08",
      "be2da38de85244ba9425301fbe53e6bf",
      "d5c40628cc1148a49ab24c413837f694",
      "a0d65ffd47d5433c987ddf8b0ec1f776",
      "b3cf4cedf2524b3385bb6ea4b89c3c13",
      "e7a946cc603b48cdaaf20744144f609e",
      "e57ac469f7d34f1c827a862795bacaed",
      "b135cd4289e8450f9b3d575b90650809",
      "14cc59ec507a478594f4604deab88681",
      "647081fd2a7f4ab28c7d19a6587dffd6",
      "887aa9d1de504ebebbdbd203b4f552a3",
      "f9f80e393b8f431480aa54064aef1c04",
      "c106c7800f934a7fb6cd109c84a2eb00",
      "8eb47590653041c98e6d7fc919f2ee1f",
      "1da71d0745754fae8aab4ae3b8f1f4c7",
      "a93bf9e787c146fb91d03d6028eecbb4"
     ]
    },
    "id": "AGH4lqLFIGbe",
    "outputId": "fecad54d-3c70-401a-a4ca-7e081d43f1e7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b630f7fbb3465fbe663933fb6170c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)0fe39/.gitattributes:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e873ee30114b29ab8c6a4241fee4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9540564e57804c329a01629c18b9d28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)83e900fe39/README.md:   0%|          | 0.00/3.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa1ca63fcc44087a6002ad58dd6ac18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)e900fe39/config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0b6d625c4d463fa1870cbda67e3832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6241e4ae3d3d434d9d39e281b48df215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2755e3f8c2432097d81518f86f3b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe8f30ae2f340abb50287854b4deb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e560bee40e4184a75bff34c55b45c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a13f92e7c414d1b960edcf231f2bad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3e9cb65017494ca25b094c4e56d855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59c747b566d46e882cc6fd0cff84fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading unigram.json:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a946cc603b48cdaaf20744144f609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)900fe39/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings_st = SentenceTransformerEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "# Otro modelo en espa√±ol que podr√≠amos usar es \"symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stA_QQldNQgu",
    "outputId": "b908959d-5efc-42e6-bc2e-849ac8a27706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incrustaciones = embeddings_st.embed_documents(documentos_a_incrustar)\n",
    "len(incrustaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ac4Gl781IDP",
    "outputId": "0856cc89-3773-4a01-aa4b-4027a5ebbf09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incrustaciones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6rUBhReNWsI"
   },
   "outputs": [],
   "source": [
    "incrustacion = embeddings_st.embed_query(documentos_a_incrustar[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tr9Td3_t1kBj",
    "outputId": "6d94afcd-c53a-40fe-deb4-570dff3704e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incrustacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ujz0WruZRgFC"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install InstructorEmbedding sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500,
     "referenced_widgets": [
      "180721fcf3ff47b0b6f40a5557384034",
      "09479a96d83a40988e931e1166ccd31d",
      "2ce416f49ad54476a3f86ec326baceaf",
      "245869f0479446d3855b98a754f5df0b",
      "51f8e3de585d42f99e69cd630b6e4e6f",
      "3fb36079bfb44add8a2be5d33872064c",
      "60938395a6724a179c3ff1535aa9ec69",
      "ae6a6b21112746e8a193dbdb186f8679",
      "74a66e7a37794d58b3667d00422fdb92",
      "830e1d6f08b345d3993708d32f5d6482",
      "0f5c46b8d8134bcaafd7d8cec193a2a0",
      "a966328d10fb4a34b162d9f8ca9b4f54",
      "6e37d2b76cfe476bb9f709f2357b4879",
      "78bccbf41a88415d9c1f8b855755d431",
      "bb070ce2e97c444c98ff08d20db67900",
      "303528a6414143eb9d7566ba3ba57f7e",
      "5804b03890f148dbaeb6b63e44804de7",
      "faefa86518e84e2991414bbef23e48ad",
      "5c23f703e566425fbc306669b84c79b3",
      "a39ec7d07ec942cc806e6dcc7046bfc7",
      "a365273e2da54b8688fac335b074d555",
      "99e80b84b25d44879a155babf94dc4bc",
      "ac2c28149a304f218d15b5a4b9967536",
      "21c853eff0d046c085d81f3e93591d8d",
      "5b54e82c758049d19465894bdfa391ad",
      "a9beceb3ce464d0fbea2ee07f83ba54f",
      "434b9b2cc1e64eb1a99d7d8b32db6d09",
      "4ae04609bcb64a1bb920401bdc0769c5",
      "1256fec27c6f499aa4d5411a65d81dec",
      "006774b5ae4247d3916c91c17d6b5c6a",
      "149c61fc94d34ac08c83579a56e6df62",
      "9838eb97ff9b4cb8ab402b710e3d7ba3",
      "51bf4a065e5a4ed4b685ecb4aa675de3",
      "3096adb2700d4fb396884257d74ed481",
      "b95db005b17a4cf5a15d2c9f10519b48",
      "63ec53d18034497681cd5e61e7fe018a",
      "d6686c8632504993baa58162713c16b1",
      "ba908eae541b4fff956545cbf458ef18",
      "f050acfbe9a74a519a8f9e49430735bb",
      "1871cc08b0d54a37bfe57f47a58382e2",
      "c405aecedd5c48a5b809f560255c0163",
      "0d6d3e8dc8cb4951b7fc5e816afdb686",
      "fc76d6f4a76f419abf094fd2ee123d56",
      "a6b9f8f44aa747c6aeb481520499be7b",
      "89698cdbde3f4b8cbd9c5e7b60dff94b",
      "602d903185df468f99c7df5a9c654241",
      "94c5d0df08c04b8fae1450f44bc22d49",
      "1c94ab95a6c54780985c274f5ecb15a5",
      "0ed3da93b4014d27a534ff2ec85c3cca",
      "8d9575dd749547b491b8682d9e9a5f02",
      "5d17d0d64a2c4b6a95c6358d30636ea1",
      "9a07077234d6496385a1577e1d065b44",
      "6c01b89ae45241b6a3d9fc0695f43ddf",
      "dc1985b892ae4e37b1e0d24fe5b2b0e5",
      "5b709cc54d5d40a4a6e5449f794396cc",
      "f0ec19726ab744c4835a51a2d09d3ed4",
      "1f15fbca175f46498a47eaf6e287d9b5",
      "54f7cda4e48a44b89f29d22f739f4a49",
      "d4418c6a967d42e19786fc4fe7422aa1",
      "d5eefb14ab514d21995b61cbc6af43c1",
      "152e040add1140999b04ef7300984b63",
      "fafb115b852e438fa4e1ed153250e420",
      "24f62300096b4cc581bc9746decbaf21",
      "cafebbc9fe2341f38b73555bb2dc598c",
      "e9d975888ce4400c97b9dfc6ac5cd2d0",
      "acbc73449d32460a95e338a06d0b61ed",
      "0afbc63d580b497b9b313bfbe3a160c6",
      "1405e99b47c4499db3361e12c2c67e8f",
      "95ea120e751a41d1b820dd10e7533b76",
      "3a8dbd217fe4486fb70b2eb7009778b5",
      "1f377e5f12704228933cb52fa166979a",
      "611ffffe50bb452e99e296d5192850f6",
      "03a7470188b748a9b263026a1205d1ce",
      "d61937ce35024ebfbfe4e9541e561b51",
      "5800bf60f016438f81ef28ab44949e2e",
      "2e3f0282a8604e658a3385899c69a8f1",
      "2d980c5f9cc947389fb1c0b8ed109fa0",
      "2a87967b1ad546b9857053745b7fa7ec",
      "c8c7100912624ac9b8e1861842df770b",
      "8fe15e6526464e3cb9893b715ffc56e3",
      "ac6d4ce664ff4a1bbf1648f35575a227",
      "069e6e79ffee4fd8b5da8ad6b286cc5a",
      "8865c81c7e9946aa833eba512aecd3f3",
      "6bf247ad06994498b7ff5583267c98ab",
      "ebfd785e25ca41e498b16c10e61153fd",
      "816652e5c8a74c9b9f47b2ca2f256c92",
      "c1f0d2995a804f9697da5805309f7e48",
      "fce2f85e9ad0479cb69cd7bff07061dc",
      "0118c30f09a442caa7a8e8fc8fe8ed54",
      "ed7aaf8733b449d08c07c7086d6e9614",
      "f7b1b43d68584fb9bb8aee523cbc721b",
      "8efd9271242b462cae853f953e701d76",
      "3b8481fb2cf9457aac8e572f55692ef8",
      "1a5cb2a70ba247fba8ae0f5856cca43d",
      "817f93d1c21a4de1b2c19a089ef1fb4a",
      "67cb1be3703d44548652fe8250c8455b",
      "ca4c7bdbaa8b4408a891c4d36182565f",
      "c96bafe2497047c3b483a65ac8cf26fb",
      "0c1c14e33f294591864a4f5919e3ac75",
      "1d62bc0b15fd46658bd92f6a1404899c",
      "fe2d834baf744d1f9939cc7a28945da7",
      "caf152592514444c88d1aa928aac1f97",
      "941ef467f1c346dca3c8075307a902e4",
      "6af445126812425aa28e29e4e5e9a45a",
      "52fea516c48c4e10a506e5e3b9fe512f",
      "a059a01244ef47f89baf868ce28e5229",
      "0f3294280ecf45e4b9a1b1ce6ec1bb46",
      "6b2e507208d3413b94e471f904afc25a",
      "654f61cdce6843688aceb28c0950396d",
      "31320299b5884d3bb860b6ae377ec563",
      "eff97ca0a28d42408f29f0c67b6ab21d",
      "d69d70f2509d448689dd53d2fa4cf499",
      "145b9376b5454c1dacfd5b389818bc9a",
      "2a4b20ce17a74cf99598ae5c4a5508d7",
      "ef95af68dbc842dea09d1abce699c190",
      "df5eaf1feb114325a0c615b48f14912a",
      "c7aa9b1a619446d692b4a40cf236886e",
      "1900811fca2a44cfa5b4ddf45d4de156",
      "f413ff106aa548f8988f35d97dd8be07",
      "212c05b13ca8461a84c1e02496d8db9b",
      "c286246e5ae1496fb6c2f60787b3f726",
      "e11ca7bf5bd64f95929c4786c9c16cf6",
      "94fe4d830073402fa0aab3a957aaa194",
      "6a8b4efe76d2463f9c8920116354f8a2",
      "9bc37f8a38754cd8ad5b023338ed6038",
      "0a30d3b826b842748eebc1cec4ee26a5",
      "9c8d0856c6d340708a24b26118d08f9e",
      "6b530e49fa7446ef853a85e287039412",
      "e3e121ae73a04392a3ede02ca4fbf8fb",
      "c9676922e2314c118971d0eaa9d1f7a1",
      "045070e574f645d498b46640c59c57be",
      "f1e25b96700d423b81a0b445fcc8698a",
      "16aff8b4712a4af6baf8b7e56c522183",
      "c5e10f1de39a48c4a53186206df4104c",
      "b991902dfd9246caaba30b53cd831fc5",
      "b835e6c389b74dcebb80fec200bf01ba",
      "60b483b38ecf43008b16cc47c72a7e5e",
      "fa39e132f57d45709ecf61cb6463a9b6",
      "23ae88dc07d0446d95ab0f5fca2e015c",
      "7b779f4950c6410483ed7e4ac88e9b46",
      "ca420e8e062f4614b9ccf2af452d3e17",
      "dcfdf88b934a4146a798ae34dfe3d720",
      "bbcf7774dd3a4e7bbae09f24d56035d1",
      "dc7f087034434858b8d03d0fac68d5df",
      "fe94c391d7f04dc5ba2c1b774c8cafd9",
      "37e422491ef8489181f6888119c87035",
      "10a9440103954924b24db26883017f1e",
      "17bde8904a184c92adc44da6100f9a9b",
      "534d17df10e44ad39b2dd541f404c794",
      "41145620a31e4585912eba804e3cf565",
      "85e9d1fffece4c18ab7e63ac443475e1",
      "1fcfcec2ce2b4694bfbf2c25240888c9",
      "5542d79a2ed949cc8e57372185aea0dc",
      "cf9f7567281643d8a246926d2d4ad59f"
     ]
    },
    "id": "h3H8C9F8P0t6",
    "outputId": "9f05a045-479b-4a04-d805-c84c494a064a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180721fcf3ff47b0b6f40a5557384034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)c7233/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a966328d10fb4a34b162d9f8ca9b4f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2c28149a304f218d15b5a4b9967536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3096adb2700d4fb396884257d74ed481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89698cdbde3f4b8cbd9c5e7b60dff94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)9fb15c7233/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec19726ab744c4835a51a2d09d3ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)b15c7233/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afbc63d580b497b9b313bfbe3a160c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a87967b1ad546b9857053745b7fa7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0118c30f09a442caa7a8e8fc8fe8ed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d62bc0b15fd46658bd92f6a1404899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff97ca0a28d42408f29f0c67b6ab21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11ca7bf5bd64f95929c4786c9c16cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)c7233/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aff8b4712a4af6baf8b7e56c522183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7f087034434858b8d03d0fac68d5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)15c7233/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# A junio de 2023 no hay modelos Instruct para espa√±ol\n",
    "embedding_instruct = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\",\n",
    "    model_kwargs={\"device\":\"cuda\"}\n",
    ")\n",
    "\n",
    "# El device podr√≠a ser cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYOir6kGQif-"
   },
   "outputs": [],
   "source": [
    "incrustaciones = embedding_instruct.embed_documents(documentos_a_incrustar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUHB4GbM2XXv",
    "outputId": "a4bfa4f5-6029-4f37-fd4c-8ca3f905218b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incrustaciones[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbtZ51ZxQjp5"
   },
   "outputs": [],
   "source": [
    "incrustacion = embedding_instruct.embed_query(documentos_a_incrustar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYw94JkJ2khf",
    "outputId": "cb88c12f-9473-4b14-e3a2-09589fe2419e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incrustacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m9mVQO1qFA2"
   },
   "source": [
    "### La importancia del tama√±o de los embeddings\n",
    "\n",
    "Los modelos de incrustaciones de texto (embeddings) son un recurso crucial en el procesamiento del lenguaje natural. Sin embargo, es importante tener en cuenta que estos modelos tienen una capacidad limitada en t√©rminos de la cantidad de tokens que pueden manejar antes de truncar los textos.\n",
    "\n",
    "Cada proveedor de modelos de incrustaciones puede tener un l√≠mite de tokens diferente y estos l√≠mites pueden variar con el tiempo. Es recomendable que consultes la documentaci√≥n actualizada del proveedor para obtener informaci√≥n precisa. Esta gu√≠a se cre√≥ a medidados de 2023 y, por lo tanto, los l√≠mites espec√≠ficos pueden haber cambiado.\n",
    "\n",
    "Para los modelos de OpenAI, por ejemplo, las actualizaciones a menudo se anuncian en blogs o en su p√°gina de modelos. Para los modelos del Hub de Hugging Face, puedes usar el m√©todo `.client` para conocer el l√≠mite de tokens. Para los modelos de Cohere, aunque no est√° especificado claramente, se recomienda mantener los textos menores a 512 tokens.\n",
    "\n",
    "#### Recomendaciones generales de tama√±o de incrustaciones\n",
    "\n",
    "Para todos los modelos de incrustaciones que manejamos, una regla general ser√≠a mantener los textos menores a 512 tokens. Esta restricci√≥n de tama√±o ayuda a garantizar que los modelos puedan procesar eficientemente el texto sin truncarlo.\n",
    "\n",
    "Existe una gran posibilidad de que los modelos de incrustaciones futuras puedan manejar contextos m√°s grandes (es decir, m√°s tokens) sin perder capacidad de procesamiento. Sin embargo, al menos hasta junio de 2023 y probablemente durante todo el a√±o 2023, la recomendaci√≥n seguir√° siendo mantener los textos dentro del l√≠mite de 512 tokens.\n",
    "\n",
    "Aunque los modelos de incrustaciones de OpenAI pueden mencionar la capacidad de manejar hasta 8192 tokens, es importante recordar que el rendimiento √≥ptimo del modelo puede no alcanzarse con textos de este tama√±o. Por lo tanto, se recomienda la cautela y el cumplimiento de la recomendaci√≥n general de 512 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRaTM4BuxQQ0",
    "outputId": "f28c1b08-a3de-402d-8abb-6ce792c77f44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(INSTRUCTOR(\n",
       "   (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
       "   (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       "   (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
       "   (3): Normalize()\n",
       " ),\n",
       " SentenceTransformer(\n",
       "   (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "   (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       " ))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_instruct.client, embeddings_st.client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2umF2UtYV3Md"
   },
   "source": [
    "## Bases de datos vectoriales\n",
    "\n",
    "Imagina que eres un bibliotecario, pero tu biblioteca consta de vectores de alta dimensi√≥n en lugar de libros, y tus usuarios son agentes de IA en lugar de humanos. Por futurista que parezca, esta es la realidad de una base de datos de vectores: un banco de memoria para la IA, dise√±ado para almacenar y recuperar datos vectoriales de alta dimensi√≥n con eficiencia y precisi√≥n. Al igual que un bibliotecario organizar√≠a y buscar√≠a libros, una base de datos de vectores proporciona un m√©todo para gestionar y encontrar vectores en un espacio de alta dimensi√≥n.\n",
    "\n",
    "En este cap√≠tulo, profundizaremos en las complejidades de las bases de datos de vectores. Desentra√±aremos su creciente importancia, entenderemos qu√© implica la data vectorial y exploraremos los aspectos pr√°cticos de las bases de datos de vectores.\n",
    "\n",
    "## El ascenso y la significancia de las bases de datos vectoriales\n",
    "\n",
    "Las bases de datos de vectores est√°n ganando prominencia en la industria tecnol√≥gica, evidenciado por las significativas inversiones en tecnolog√≠as de bases de datos de vectores en los √∫ltimos a√±os. Algunos ejemplos incluyen la inversi√≥n de $28M de Pinecone, la ronda semilla de $10M de LangChain y la ronda semilla de $18M de Chroma. El flujo de dinero habla mucho sobre el futuro y el potencial de las bases de datos de vectores en la IA.\n",
    "\n",
    "La evoluci√≥n de las tecnolog√≠as de gesti√≥n de datos puede asemejarse a un r√≠o: siempre fluyendo, adapt√°ndose continuamente al paisaje. Desde esquemas r√≠gidos y estructurados en bases de datos relacionales hasta el manejo flexible de datos no estructurados o semi-estructurados en bases de datos NoSQL, la gesti√≥n de datos es un dominio en flujo, evolucionando para satisfacer nuestras crecientes necesidades de datos.\n",
    "\n",
    "La aparici√≥n de las bases de datos de vectores es el √∫ltimo desarrollo en este viaje. Estas bases de datos abordan los desaf√≠os de gestionar y consultar datos vectoriales de alta dimensi√≥n, tambi√©n conocidos como \"incrustaciones de vectores\".\n",
    "\n",
    "### El rol de las bases de datos vectoriales\n",
    "\n",
    "Las bases de datos de vectores, tambi√©n conocidas como bases de datos de b√∫squeda de similitud o bases de datos de b√∫squeda del vecino m√°s cercano, est√°n especialmente dise√±adas para almacenar y recuperar incrustaciones de vectores. Estas bases de datos pueden realizar operaciones como encontrar elementos similares a un vector dado o buscar elementos que cumplan con ciertos criterios de similitud. Imagina poder preguntarle a tu base de datos, \"encu√©ntrame m√°s palabras como 'alegre'\" y obtener respuestas como 'contento', 'feliz' y 'jubiloso'. Las bases de datos tradicionales no est√°n dise√±adas para este tipo de consultas, donde las bases de datos de vectores destacan.\n",
    "\n",
    "Con los conceptos b√°sicos cubiertos, ahora estamos preparados para adentrarnos m√°s en el mundo de la gesti√≥n de datos vectoriales. En las siguientes secciones, exploraremos c√≥mo integrar las bases de datos de vectores usando Python y compararemos algunas de las plataformas l√≠deres como Pinecone, Chroma y LangChain.\n",
    "\n",
    "Las bases de datos tienen una rica historia, evolucionando desde simples registros hasta estructuras complejas capaces de capturar, consultar y analizar informaci√≥n a lo largo del tiempo. Nos encontramos en un momento crucial, ya que el auge de la IA generativa se entrelaza con nuestras herramientas de gesti√≥n de datos, creando nuevos potenciales y desaf√≠os.\n",
    "\n",
    "Los vectores representan 'objetos' de datos, llevando informaci√≥n sobre el tiempo, el lugar, los atributos y m√°s, permiti√©ndonos enriquecer nuestros datos. Ayudan a rastrear tendencias temporales, permiti√©ndonos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HukX2N_iZTar"
   },
   "source": [
    "### Chroma\n",
    "\n",
    "Chroma es un proyecto de c√≥digo abierto que provee una base de datos espec√≠ficamente dise√±ada para guardar y consultar incrustaciones, en conjunci√≥n con sus respectivos metadatos. Fue dise√±ada para trabajar con Modelos Grandes de Lenguaje (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxgba2hpZU-E"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKcp1R4UBh6A"
   },
   "source": [
    "Embed and store the texts. Supplying a persist_directory will store the embeddings on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "644WFTxQBfUR"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "NOMBRE_INDICE_CHROMA = \"instruct-embeddings-public-crypto\"\n",
    "\n",
    "vectorstore_chroma = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_instruct,\n",
    "    persist_directory=NOMBRE_INDICE_CHROMA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4WpCQ8V5xxD",
    "outputId": "1b3ad81f-adfc-4880-98d0-1755f4cc6d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.embeddings.huggingface.HuggingFaceInstructEmbeddings"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZKGJ0ACDgrJ"
   },
   "source": [
    "Hacer que nuestra vectorstore persista en nuestro disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjWGPHsoB3X2"
   },
   "outputs": [],
   "source": [
    "vectorstore_chroma.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiEh19f56m5z"
   },
   "outputs": [],
   "source": [
    "vectorstore_chroma = Chroma(\n",
    "    persist_directory=NOMBRE_INDICE_CHROMA,\n",
    "    embedding_function=embedding_instruct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H7qqF-UDuXX"
   },
   "source": [
    "Podemos cargar la base de datos persistente desde el disco y usarla en cualquier momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO91LF_zB_Xc"
   },
   "outputs": [],
   "source": [
    "query = \"What is public key cryptography?\"\n",
    "\n",
    "docs = vectorstore_chroma.similarity_search_with_score(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqM5iPNpDGFv",
    "outputId": "681ad41e-5b64-46a9-a7da-b5f58719926e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMuWYuVJDHhM",
    "outputId": "b0ada069-159f-4dc0-90de-1de1b2033390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(page_content='The First Ten Years of Public-Key \\nCryptography \\nWH lTFl ELD DI FFlE \\nInvited Paper \\nPublic-key cryptosystems separate the capacities for encryption \\nand decryption so that 7) many people can encrypt messages in \\nsuch a way that only one person can read them, or 2) one person \\ncan encrypt messages in such a way that many people can read \\nthem. This separation allows important improvements in the man- \\nagement of cryptographic keys and makes it possible to ‚Äòsign‚Äô a \\npurely digital message.', metadata={'source': './public_key_cryptography.pdf', 'page': 0}),\n",
       " 0.18773773312568665)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpr5y1OeQG8l"
   },
   "source": [
    "#### Creando un Retriever\n",
    "\n",
    "Un retriever es una herramienta esencial para realizar b√∫squedas dentro de nuestros 'vectorstores'. En t√©rminos sencillos, un retriever es algo as√≠ como un \"buscador\" o \"recuperador\".\n",
    "\n",
    "El retriever permite definir el n√∫mero de documentos relevantes que queremos obtener como resultado de nuestras b√∫squedas. Esto se puede ajustar mediante el argumento `search_kwargs` en el m√©todo `.as_retriever()`.\n",
    "\n",
    "Es posible especificar la estrategia que se usar√° para encontrar los documentos relevantes usando `search_type`. Este par√°metro puede tomar dos valores: \"similarity\" y \"exact_match\".\n",
    "\n",
    "**Similaridad (\"similarity\")**: Busca documentos que sean similares a la consulta. Los documentos se clasifican seg√∫n su puntuaci√≥n de similitud, donde una puntuaci√≥n m√°s baja indica una mejor coincidencia.\n",
    "\n",
    "**Coincidencia Exacta (\"exact_match\")**: Busca documentos que coincidan exactamente con la consulta. No considera la similitud entre la consulta y los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLqbFf08Ih6i"
   },
   "outputs": [],
   "source": [
    "retriever_chroma = vectorstore_chroma.as_retriever(\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5dkRoCt_G_-",
    "outputId": "c910bd1e-bbc3-496c-a528-f8bc313a5b4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='purely digital message. \\nPublic key cryptography was discovered in the Spring of 1975 \\nand has followed a surprising course. Although diverse systems \\nwere proposed early on, the ones that appear both practical and \\nsecure today are all very closely related and the search for new and \\ndifferent ones has met with little success. Despite this reliance on \\na limited mathematical foundation public-key cryptography is rev- \\nolutionizing communication security by making possible secure', metadata={'source': './public_key_cryptography.pdf', 'page': 0}),\n",
       " Document(page_content='The First Ten Years of Public-Key \\nCryptography \\nWH lTFl ELD DI FFlE \\nInvited Paper \\nPublic-key cryptosystems separate the capacities for encryption \\nand decryption so that 7) many people can encrypt messages in \\nsuch a way that only one person can read them, or 2) one person \\ncan encrypt messages in such a way that many people can read \\nthem. This separation allows important improvements in the man- \\nagement of cryptographic keys and makes it possible to ‚Äòsign‚Äô a \\npurely digital message.', metadata={'source': './public_key_cryptography.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chroma.get_relevant_documents(\"What are the recent advances on public key cryptography?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSZgQQgHNiFT"
   },
   "source": [
    "#### Creando una cadena para preguntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-douoXbf_5Gw"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJOLPPtZC5ob"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_chain_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever_chroma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HntS78wfDy9l",
    "outputId": "ee9e322f-5fcc-4617-b8f3-363866e157a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the relevance of public key crypto?',\n",
       " 'answer': 'Public key cryptography allows for encryption and decryption using inverse pairs of keys, making it possible for secure communication without the need for a shared secret key. It also allows for the exchange of new session keys and helps to prevent the use of compromised keys to read earlier traffic. \\n',\n",
       " 'sources': './public_key_cryptography.pdf'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the relevance of public key crypto?\"\n",
    "respuesta = qa_chain_with_sources(query)\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAGftKPMKf5G",
    "outputId": "f07f87d1-22fd-4bf7-b5b9-80c1b28275ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is crypto?',\n",
       " 'answer': 'Crypto refers to public-key cryptography, which is a cryptosystem in which keys come in inverse pairs and each pair of keys has two properties. It has had a significant impact on communication security and has given cryptographers a systematic means of addressing a broad range of security objectives. \\n',\n",
       " 'sources': './public_key_cryptography.pdf'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is crypto?\"\n",
    "respuesta = qa_chain_with_sources(query)\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfGrVzGg8n8G"
   },
   "source": [
    "Todos estos son resultados buenos y relevantes. Pero, ¬øqu√© podemos hacer con ellos? Existen diversas tareas que podemos realizar, pero una de las m√°s interesantes (y muy bien soportada por LangChain) es la \"Generaci√≥n de Preguntas y Respuestas\" o GQA.\n",
    "\n",
    "En la Generaci√≥n de Preguntas y Respuestas (GQA), un modelo de lenguaje se utiliza para generar respuestas a preguntas basadas en un texto dado. Esto puede ser particularmente √∫til en una variedad de aplicaciones, desde chatbots inteligentes que pueden responder a preguntas basadas en manuales de usuario o documentaci√≥n de productos, hasta motores de b√∫squeda m√°s avanzados que pueden responder a preguntas en lugar de simplemente proporcionar una lista de documentos relevantes.\n",
    "\n",
    "Los modelos de GQA trabajan generando representaciones vectoriales de los documentos y las consultas, y luego usan medidas de similitud (como las mencionadas anteriormente) para identificar los documentos o partes de documentos que son m√°s relevantes para la consulta. Esto va m√°s all√° de la simple b√∫squeda de palabras clave, ya que los modelos pueden capturar la sem√°ntica y el contexto de las consultas y documentos, lo que les permite responder preguntas m√°s complejas y proporcionar respuestas m√°s precisas y detalladas.\n",
    "\n",
    "En GQA, tomamos la consulta como una pregunta que debe ser respondida por un LLM, pero el LLM debe responder la pregunta bas√°ndose en la informaci√≥n que se le devuelve desde el almac√©n de vectores.\n",
    "\n",
    "Para hacer esto, inicializamos un objeto RetrievalQA, pero antes creamos un objeto de tipo llm y en este caso usaremos el modelo `gpt-3.5-turbo` de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rkp3ITwB_V4W"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
